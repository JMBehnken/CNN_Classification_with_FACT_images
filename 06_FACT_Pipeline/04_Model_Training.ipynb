{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training the Model and tuning the Hyperparameter\n",
    "\n",
    "A randomized Gridsearch with early-stopping will be executet in this script. The results will be append to a csv-file for further analisys. The best Model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mc_data_path = '/fhgfs/users/jbehnken/01_Data/01_MC_Data' # Path to preprocessed data\n",
    "num_files = 100 # Number of files to load - 1 file = 1000 events\n",
    "events_in_validation = 10000\n",
    "number_of_nets = 10\n",
    "\n",
    "save_model_path = '/fhgfs/users/jbehnken/01_Data/04_Models'\n",
    "model_name = 'ccff'\n",
    "title_name = 'Optimizing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_paths = os.listdir(save_model_path)\n",
    "for path in file_paths:\n",
    "    name = '_' + model_name\n",
    "    if path.endswith(name):\n",
    "        correct_path = path \n",
    "\n",
    "if 'correct_path' in locals():\n",
    "    folder_path = os.path.join(save_model_path, correct_path)\n",
    "else:\n",
    "    folder_number = len(os.listdir(save_model_path))+1\n",
    "    folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "    os.mkdir(folder_path)\n",
    "    \n",
    "    with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Learning_Rate','Batch_Size','Patch_Size','Depth','Hidden_Nodes','Accuracy','Auc','Steps', 'Early_Stopped','Time', 'Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading randomized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(mc_data_path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Randomizing the files to load\n",
    "loading_files = os.listdir(mc_data_path)\n",
    "np.random.shuffle(loading_files)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, loading_files[:num_files])\n",
    "pics, labs = zip(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Splitting data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "\n",
    "valid_dataset = pic[p][:events_in_validation]\n",
    "valid_labels = lab[p][:events_in_validation]\n",
    "train_dataset = pic[p][events_in_validation:]\n",
    "train_labels = lab[p][events_in_validation:]\n",
    "del p, pic, lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St_auc: 0.0, sc: 0,val: 78.60999703407288, Step: 0\n",
      "St_auc: 0.781628429889679, sc: 0,val: 81.23999834060669, Step: 1000\n",
      "St_auc: 0.8093738555908203, sc: 0,val: 85.37001013755798, Step: 2000\n",
      "St_auc: 0.8370044231414795, sc: 0,val: 87.71001100540161, Step: 3000\n",
      "St_auc: 0.8574053645133972, sc: 0,val: 88.02000284194946, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 21.299998462200165, Step: 0\n",
      "St_auc: 0.21292130649089813, sc: 0,val: 84.59000587463379, Step: 1000\n",
      "St_auc: 0.3946773409843445, sc: 0,val: 87.63000965118408, Step: 2000\n",
      "St_auc: 0.5366515517234802, sc: 0,val: 88.33000659942627, Step: 3000\n",
      "St_auc: 0.6242143511772156, sc: 0,val: 87.95000910758972, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 21.279998123645782, Step: 0\n",
      "St_auc: 0.2128276377916336, sc: 0,val: 84.66001152992249, Step: 1000\n",
      "St_auc: 0.3952914774417877, sc: 0,val: 87.0400071144104, Step: 2000\n",
      "St_auc: 0.5370321869850159, sc: 0,val: 87.99000978469849, Step: 3000\n",
      "St_auc: 0.6239413022994995, sc: 0,val: 88.36001753807068, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 29.879996180534363, Step: 0\n",
      "St_auc: 0.2676509618759155, sc: 0,val: 82.68001079559326, Step: 1000\n",
      "St_auc: 0.7014184594154358, sc: 0,val: 84.4599962234497, Step: 2000\n",
      "St_auc: 0.7998167872428894, sc: 0,val: 85.89000701904297, Step: 3000\n",
      "St_auc: 0.8426038026809692, sc: 0,val: 87.12001442909241, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 21.289998292922974, Step: 0\n",
      "St_auc: 0.21282130479812622, sc: 0,val: 85.05000472068787, Step: 1000\n",
      "St_auc: 0.394488662481308, sc: 0,val: 86.29001379013062, Step: 2000\n",
      "St_auc: 0.5365570783615112, sc: 0,val: 88.27999830245972, Step: 3000\n",
      "St_auc: 0.6233352422714233, sc: 0,val: 88.17000389099121, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 78.70000004768372, Step: 0\n",
      "St_auc: 0.7870000004768372, sc: 0,val: 84.84000563621521, Step: 1000\n",
      "St_auc: 0.819593071937561, sc: 0,val: 88.02000284194946, Step: 2000\n",
      "St_auc: 0.8509436845779419, sc: 0,val: 88.15001249313354, Step: 3000\n",
      "St_auc: 0.8697462677955627, sc: 0,val: 87.88001537322998, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 78.70000004768372, Step: 0\n",
      "St_auc: 0.7870000004768372, sc: 0,val: 83.85000824928284, Step: 1000\n",
      "St_auc: 0.8172589540481567, sc: 0,val: 87.24000453948975, Step: 2000\n",
      "St_auc: 0.8471999168395996, sc: 0,val: 87.57001161575317, Step: 3000\n",
      "St_auc: 0.8660340905189514, sc: 0,val: 87.55999803543091, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 78.70000004768372, Step: 0\n",
      "St_auc: 0.7870000004768372, sc: 0,val: 84.57000255584717, Step: 1000\n",
      "St_auc: 0.8178784847259521, sc: 0,val: 87.63000965118408, Step: 2000\n",
      "St_auc: 0.8482858538627625, sc: 0,val: 87.94000148773193, Step: 3000\n",
      "St_auc: 0.8679051399230957, sc: 0,val: 88.1100058555603, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 78.70000004768372, Step: 0\n",
      "St_auc: 0.7870000004768372, sc: 0,val: 84.17000770568848, Step: 1000\n",
      "St_auc: 0.819287896156311, sc: 0,val: 86.90000772476196, Step: 2000\n",
      "St_auc: 0.8494704365730286, sc: 0,val: 88.49000930786133, Step: 3000\n",
      "St_auc: 0.8688554167747498, sc: 0,val: 88.24000358581543, Step: 4000\n",
      "St_auc: 0.0, sc: 0,val: 21.34999781847, Step: 0\n",
      "St_auc: 0.21338440477848053, sc: 0,val: 85.09000539779663, Step: 1000\n",
      "St_auc: 0.39712148904800415, sc: 0,val: 87.87001371383667, Step: 2000\n",
      "St_auc: 0.5432354211807251, sc: 0,val: 87.96000480651855, Step: 3000\n",
      "St_auc: 0.629180371761322, sc: 0,val: 88.68001699447632, Step: 4000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter for the model (fit manually)\n",
    "num_labels = 2 # gamma or proton\n",
    "num_channels = 1 # it is a greyscale image\n",
    "\n",
    "num_steps = np.random.randint(5, 50, size=number_of_nets)*1000 +1 # 5001 - 50001\n",
    "learning_rate = [0.001] * number_of_nets # 0.001\n",
    "batch_size = np.random.randint(64, 256, size=number_of_nets) # 64 - 256\n",
    "patch_size = np.random.randint(0, 2, size=number_of_nets)*2+3 # 3 / 5\n",
    "depth = np.random.randint(8, 32, size=number_of_nets) # 8 - 32\n",
    "num_hidden = np.random.randint(8, 128, size=number_of_nets) # 8 - 128\n",
    "\n",
    "hyperparameter = zip(num_steps, learning_rate, batch_size, patch_size, depth, num_hidden)\n",
    "\n",
    "df = pd.read_csv(os.path.join(folder_path, model_name+'_Hyperparameter.csv'))\n",
    "if len(df['Auc']) > 0:\n",
    "    best_auc = df['Auc'].max()\n",
    "else:\n",
    "    best_auc = 0\n",
    "\n",
    "for num_steps, learning_rate, batch_size, patch_size, depth, num_hidden in hyperparameter:\n",
    "    try:\n",
    "        # Path to logfiles and correct file name\n",
    "        start = time.time()\n",
    "        LOGDIR = '/fhgfs/users/jbehnken/tf_logs/small_logs'\n",
    "        logcount = str(len(os.listdir(LOGDIR)))\n",
    "        hparams = 'bs={}_ps={}_d={}_nh={}_ns={}'.format(batch_size, patch_size, depth, num_hidden, num_steps)\n",
    "\n",
    "        # Build the graph\n",
    "        gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.33)\n",
    "        session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=18, inter_op_parallelism_threads=18)\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "\n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='training_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='training_labels')\n",
    "\n",
    "        #tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_valid_dataset = tf.constant(valid_dataset, name='validation_data')\n",
    "        tf_valid_labels = tf.constant(valid_labels, name='validation_labels')\n",
    "\n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('conv2d_1'):\n",
    "            layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1), name='W')\n",
    "            layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            #tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            #tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('conv2d_2'):\n",
    "            layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, 2*depth], stddev=0.1), name='W')\n",
    "            layer2_biases = tf.Variable(tf.constant(1.0, shape=[2*depth]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            #tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            #tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('reshape'):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Third layer is a dense layer\n",
    "        with tf.name_scope('fc_1'):\n",
    "            layer3_weights = tf.Variable(tf.truncated_normal([12*12*2*depth, num_hidden], stddev=0.1), name='W')\n",
    "            layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "            #tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            #tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Fourth layer is a dense output layer\n",
    "        with tf.name_scope('fc_2'):\n",
    "            layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name='W')\n",
    "            layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "            #tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            #tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            #tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('prediction'):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(train_prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('batch_accuracy', accuracy)\n",
    "\n",
    "        with tf.name_scope('validation'):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME')  + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_2.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            valid_prediction = tf.nn.softmax(tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(valid_labels, 1))\n",
    "            valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('validation_accuracy', valid_accuracy)                        \n",
    "\n",
    "        with tf.name_scope('auc'):\n",
    "            auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "            tf.summary.scalar('validation_auc_0', auc[0])\n",
    "            #tf.summary.scalar('validation_auc_1', auc[1])\n",
    "\n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(LOGDIR+'/'+logcount+hparams)\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        # Iterating over num_setps \n",
    "        for step in range(num_steps):\n",
    "            # Computing the offset to move over the training dataset\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Getting the batchdata\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            # Getting the batchlabels\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Creating a feed_dict to train the model on in this step\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            # Train the model for this step\n",
    "            _, l, predictions = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            # Updating the output to stay in touch with the training process\n",
    "            if (step % 1000 == 0):\n",
    "                [acc, val, auc_val, s] = sess.run([accuracy, valid_accuracy, auc, summ], feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "                #print('Auc: %.2f, %.2f' % (auc_val[0], auc_val[1]))\n",
    "                #writer.add_summary(s, step)\n",
    "\n",
    "                if step == 0:\n",
    "                    stopping_auc = 0.0\n",
    "                    sink_count = 0\n",
    "                else:\n",
    "                    if auc_val[0] > stopping_auc:\n",
    "                        stopping_auc = auc_val[0]\n",
    "                        sink_count = 0\n",
    "                        if auc_val[0] > best_auc:\n",
    "                            saver.save(sess, os.path.join(folder_path, model_name))\n",
    "                            best_auc = auc_val[0]\n",
    "                    else:\n",
    "                        sink_count += 1\n",
    "                print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, val*100, step))\n",
    "                if sink_count == 5:\n",
    "                    break   \n",
    "\n",
    "        sess.close()\n",
    "        dauer = time.time() - start\n",
    "        early_stopped = True if step < num_steps-1 else False\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([learning_rate, batch_size, patch_size, depth, num_hidden, val*100, stopping_auc, step, early_stopped, dauer, title_name])\n",
    "    except:\n",
    "        sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
