{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Retrieving all MC-Data and storing it in matrices\n",
    "\n",
    "To reduce computation time the data will be preprocessed to fit into the CNN. First of all the photon arrival times are loaded from the server and are shaped to fit into a flat 2d-matrix.\n",
    "\n",
    "Execute this script only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Important variables\n",
    "mc_data_path = '/net/big-tank/POOL/projects/fact/simulation/photon_stream/fact_tools/v.0.18.0/'\n",
    "id_position_path = '/home/jbehnken/06_FACT_Pipeline/01_hexagonal_position_dict.p'\n",
    "temporary_path = '/fhgfs/users/jbehnken/01_Data/99_Temporary'\n",
    "processed_data_path = '/fhgfs/users/jbehnken/01_Data/01_MC_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getMetadata():\n",
    "    '''\n",
    "    Gathers the file paths of the training data\n",
    "    '''\n",
    "    # Iterate over every file in the subdirs and check if it has the right file extension\n",
    "    file_paths = [os.path.join(dirPath, file) for dirPath, dirName, fileName in os.walk(os.path.expanduser(mc_data_path)) for file in fileName if '.json' in file]\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 46, 45, 1)).astype(np.float32)\n",
    "    labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Every gzip-file will be opened and the contained information will be reshaped with the mapping of the hexagonal-position dictionary. Afterwards the images are ready to ft into the CNN. Batches of 1000 events will be stored in separated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_paths = getMetadata()\n",
    "id_position = pickle.load(open(id_position_path, \"rb\"))\n",
    "\n",
    "data = []\n",
    "num = 0\n",
    "for path in file_paths:\n",
    "    with gzip.open(path) as file:\n",
    "        # Gamma=True, Proton=False\n",
    "        label = True if 'gamma' in path else False\n",
    "        \n",
    "        for line in file:\n",
    "            event_photons = json.loads(line.decode('utf-8'))['PhotonArrivals_500ps']\n",
    "            \n",
    "            input_matrix = np.zeros([46,45])\n",
    "            for i in range(1440):\n",
    "                x, y = id_position[i]\n",
    "                input_matrix[int(x)][int(y)] = len(event_photons[i])\n",
    "            \n",
    "            data.append([input_matrix, label])\n",
    "            \n",
    "            if len(data)%1000 == 0:\n",
    "                pic, lab = zip(*data)\n",
    "                pic, lab = reformat(np.array(pic), np.array(lab))\n",
    "                data_dict={'Image':pic, 'Label':lab}\n",
    "                \n",
    "                with gzip.open( temporary_path + \"/PhotonArrivals_500ps_\"+str(num)+\".p\", \"wb\" ) as data_file:\n",
    "                    pickle.dump(data_dict, data_file)\n",
    "                data = []\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Randomizing and standardizing the formated data\n",
    "\n",
    "All files will be loaded into memory, then they will be shuffled, standardized and stored again into the same data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(temporary_path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, os.listdir(temporary_path))\n",
    "pics, labs = zip(*data)\n",
    "del data, p\n",
    "\n",
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "\n",
    "# Values to standardize the data\n",
    "mean = np.mean(pic)\n",
    "std = np.std(pic)\n",
    "print(mean, std)\n",
    "\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "all_pics = pic[p]\n",
    "all_labels = lab[p]\n",
    "del p, pic, lab\n",
    "\n",
    "def save_data(i):\n",
    "    pics_batch = all_pics[(i-1)*1000:i*1000]\n",
    "    labels_batch = all_labels[(i-1)*1000:i*1000]\n",
    "    \n",
    "    data_dict={'Image':(pics_batch-mean)/std, 'Label':labels_batch}\n",
    "    with gzip.open(processed_data_path + '/PhotonArrivals_500ps_{}.p'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "        \n",
    "num_files = len(os.listdir(temporary_path))\n",
    "p = Pool()\n",
    "data = p.map(save_data, range(1,num_files+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "mean: 1.24904\n",
    "\n",
    "std: 2.36506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Result\n",
    "\n",
    "The 2.422.000 events are standardized, randomized and saved to the disc in files containing 1000 single events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
