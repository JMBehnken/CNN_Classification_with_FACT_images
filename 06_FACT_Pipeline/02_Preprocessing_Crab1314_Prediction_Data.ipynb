{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Crab1314 to predict the labels\n",
    "\n",
    "This script will load a predefined set of runs and store them in a single h5 file. \n",
    "The images will be processed to fit into the CNN and also the eventids will be saved too.\n",
    "\n",
    "Only execute this script once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important variables\n",
    "crab1314_path = '/net/big-tank/POOL/projects/fact/photon-stream/pass4/phs/'\n",
    "crab1314_runs = '/home/jbehnken/06_FACT_Pipeline/02_Crab1314_runs.csv'\n",
    "id_position_path = '/home/jbehnken/06_FACT_Pipeline/01_hexagonal_position_dict.p'\n",
    "crab1314_images_path = '/fhgfs/users/jbehnken/01_Data/02_Crab_Prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format dataset to fit into tensorflow\n",
    "def reformat(dataset):\n",
    "    return dataset.reshape((-1, 46, 45, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchYielder():\n",
    "    paths = []\n",
    "    # Paths to the runs to be processed\n",
    "    with open(crab1314_runs) as file:\n",
    "        for line in file:\n",
    "            # Storing the path to every run file\n",
    "            l = line.split('\\t')\n",
    "            path = crab1314_path + l[0][:4]+'/' + l[0][4:6]+'/' + l[0][6:8]+'/' + l[0][:4]+l[0][4:6]+l[0][6:8]+'_'+l[1].strip()+'.phs.jsonl.gz'\n",
    "            paths.append(path)\n",
    "\n",
    "    # Load mapping-dict to switch from hexagonal to matrix\n",
    "    id_position = pickle.load(open(id_position_path, \"rb\"))\n",
    "\n",
    "    for file in paths:\n",
    "        try:\n",
    "            with gzip.open(file) as f:\n",
    "                data = []\n",
    "\n",
    "                for line in f:\n",
    "                    line_data = json.loads(line.decode('utf-8'))\n",
    "\n",
    "                    event_photons = line_data['PhotonArrivals_500ps']\n",
    "                    night = line_data['Night']\n",
    "                    run = line_data['Run']\n",
    "                    event = line_data['Event']\n",
    "                    zd_deg = line_data['Zd_deg']\n",
    "                    az_deg = line_data['Az_deg']\n",
    "                    trigger = line_data['Trigger']\n",
    "\n",
    "                    input_matrix = np.zeros([46,45])\n",
    "                    for i in range(1440):\n",
    "                        x, y = id_position[i]\n",
    "                        input_matrix[int(x)][int(y)] = len(event_photons[i])\n",
    "                    data.append([input_matrix, night, run, event, zd_deg, az_deg, trigger])\n",
    "            yield data\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the datatype to np-arrays\n",
    "def batchFormatter(batch):\n",
    "    pic, night, run, event, zd_deg, az_deg, trigger = zip(*batch)\n",
    "    pic = reformat(np.array(pic))\n",
    "    night = np.array(night)\n",
    "    run = np.array(run)\n",
    "    event = np.array(event)\n",
    "    zd_deg = np.array(zd_deg)\n",
    "    az_deg = np.array(az_deg)\n",
    "    trigger = np.array(trigger)\n",
    "    return (pic, night, run, event, zd_deg, az_deg, trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the batchYielder to concatenate every batch and store it into a single h5 file\n",
    "\n",
    "gen = batchYielder()\n",
    "batch = next(gen)\n",
    "pic, night, run, event, zd_deg, az_deg, trigger = batchFormatter(batch)\n",
    "row_count = trigger.shape[0]\n",
    "\n",
    "with h5py.File(crab1314_images_path + '/Crab1314_Images.h5', 'w') as hdf:\n",
    "    maxshape_pic = (None,) + pic.shape[1:]\n",
    "    dset_pic = hdf.create_dataset('Image', shape=pic.shape, maxshape=maxshape_pic, chunks=pic.shape, dtype=pic.dtype)\n",
    "    maxshape_night = (None,) + night.shape[1:]\n",
    "    dset_night = hdf.create_dataset('Night', shape=night.shape, maxshape=maxshape_night, chunks=night.shape, dtype=night.dtype)\n",
    "    maxshape_run = (None,) + run.shape[1:]\n",
    "    dset_run = hdf.create_dataset('Run', shape=run.shape, maxshape=maxshape_run, chunks=run.shape, dtype=run.dtype)\n",
    "    maxshape_event = (None,) + event.shape[1:]\n",
    "    dset_event = hdf.create_dataset('Event', shape=event.shape, maxshape=maxshape_event, chunks=event.shape, dtype=event.dtype)\n",
    "    maxshape_zd_deg = (None,) + zd_deg.shape[1:]\n",
    "    dset_zd_deg = hdf.create_dataset('Zd_deg', shape=zd_deg.shape, maxshape=maxshape_zd_deg, chunks=zd_deg.shape, dtype=zd_deg.dtype)\n",
    "    maxshape_az_deg = (None,) + az_deg.shape[1:]\n",
    "    dset_az_deg = hdf.create_dataset('Az_deg', shape=az_deg.shape, maxshape=maxshape_az_deg, chunks=az_deg.shape, dtype=az_deg.dtype)\n",
    "    maxshape_trigger = (None,) + trigger.shape[1:]\n",
    "    dset_trigger = hdf.create_dataset('Trigger', shape=trigger.shape, maxshape=maxshape_trigger, chunks=trigger.shape, dtype=trigger.dtype)\n",
    "    \n",
    "    dset_pic[:] = pic\n",
    "    dset_night[:] = night\n",
    "    dset_run[:] = run\n",
    "    dset_event[:] = event\n",
    "    dset_zd_deg[:] = zd_deg\n",
    "    dset_az_deg[:] = az_deg\n",
    "    dset_trigger[:] = trigger\n",
    "    \n",
    "    for batch in gen:\n",
    "        pic, night, run, event, zd_deg, az_deg, trigger = batchFormatter(batch)\n",
    "        \n",
    "        dset_pic.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_night.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_run.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_event.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_zd_deg.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_az_deg.resize(row_count + trigger.shape[0], axis=0)\n",
    "        dset_trigger.resize(row_count + trigger.shape[0], axis=0)\n",
    "        \n",
    "        dset_pic[row_count:] = pic\n",
    "        dset_night[row_count:] = night\n",
    "        dset_run[row_count:] = run\n",
    "        dset_event[row_count:] = event\n",
    "        dset_zd_deg[row_count:] = zd_deg\n",
    "        dset_az_deg[row_count:] = az_deg\n",
    "        dset_trigger[row_count:] = trigger\n",
    "        \n",
    "        row_count += trigger.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Crab1314 images are preprocessed to fit right into the CNN. The h5 file also contains the eventids to merge the prediction with the theta-values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
