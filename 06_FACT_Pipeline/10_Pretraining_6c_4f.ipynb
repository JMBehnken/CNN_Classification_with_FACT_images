{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import h5py\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFolders(model_name, save_model_path):\n",
    "    # Iterates over all existing models and chooses the right folder to save everything \n",
    "    file_paths = os.listdir(save_model_path)\n",
    "    for path in file_paths:\n",
    "        name = '_' + model_name\n",
    "        if path.endswith(name):\n",
    "            correct_path = path \n",
    "\n",
    "    # Creates missing folders or chooses the right one to append new data to\n",
    "    if 'correct_path' in locals():\n",
    "        folder_path = os.path.join(save_model_path, correct_path)\n",
    "    else:\n",
    "        folder_number = len(os.listdir(save_model_path))+1\n",
    "        folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "        # Creates the csv to save every models performance in\n",
    "        c_count = model_name.count('c')\n",
    "        depth_names = []\n",
    "        for i in range(c_count):\n",
    "            depth_names.append('Depth_{}'.format(i+1))\n",
    "        columns = ['Learning_Rate','Batch_Size','Patch_Size']\n",
    "        columns.extend(depth_names)\n",
    "        columns.extend(['Accuracy','Auc','Pretraining_Steps', 'Title'])\n",
    "\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaYielder(path_mc_images):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        keys = list(f.keys())\n",
    "        events = []\n",
    "        for key in keys:\n",
    "            events.append(len(f[key]))\n",
    "            \n",
    "    gamma_anteil = events[0]/np.sum(events)\n",
    "    hadron_anteil = events[1]/np.sum(events)\n",
    "    \n",
    "    gamma_count = int(round(num_events*gamma_anteil))\n",
    "    hadron_count = int(round(num_events*hadron_anteil))\n",
    "    \n",
    "    return gamma_anteil, hadron_anteil, gamma_count, hadron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchYielder(path_mc_images):\n",
    "    gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder(path_mc_images)\n",
    "\n",
    "    gamma_batch_size = int(round(batch_size*gamma_anteil))\n",
    "    hadron_batch_size = int(round(batch_size*hadron_anteil))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        gamma_offset = (step * gamma_batch_size) % (gamma_count - gamma_batch_size)\n",
    "        hadron_offset = (step * hadron_batch_size) % (hadron_count - hadron_batch_size)\n",
    "\n",
    "        with h5py.File(path_mc_images, 'r') as f:\n",
    "            gamma_data = f['Gamma'][gamma_offset:(gamma_offset + gamma_batch_size), :, :, :]\n",
    "            hadron_data = f['Hadron'][hadron_offset:(hadron_offset + hadron_batch_size), :, :, :]\n",
    "\n",
    "        batch_data = np.concatenate((gamma_data, hadron_data), axis=0)\n",
    "        labels = np.array([True]*gamma_batch_size+[False]*hadron_batch_size)\n",
    "        batch_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        gamma_size = int(round(events_in_validation_and_testing*gamma_anteil))\n",
    "        hadron_size = int(round(events_in_validation_and_testing*hadron_anteil))\n",
    "\n",
    "        gamma_valid_data = f['Gamma'][gamma_count:(gamma_count+gamma_size), :, :, :]\n",
    "        hadron_valid_data = f['Hadron'][hadron_count:(hadron_count+hadron_size), :, :, :]\n",
    "\n",
    "        valid_dataset = np.concatenate((gamma_valid_data, hadron_valid_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        valid_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "\n",
    "        gamma_test_data = f['Gamma'][(gamma_count+gamma_size):(gamma_count+2*gamma_size), :, :, :]\n",
    "        hadron_test_data = f['Hadron'][(hadron_count+hadron_size):(hadron_count+2*hadron_size), :, :, :]\n",
    "\n",
    "        test_dataset = np.concatenate((gamma_test_data, hadron_test_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        test_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "        \n",
    "    return valid_dataset, valid_labels, test_dataset, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestAuc(folder_path, architecture):\n",
    "    # Loading the existing runs to find the best auc untill now. Only a model with a better auc will be saved\n",
    "    df = pd.read_csv(os.path.join(folder_path, architecture+'_Hyperparameter.csv'))\n",
    "    if len(df['Auc']) > 0:\n",
    "        best_auc = df['Auc'].max()\n",
    "    else:\n",
    "        best_auc = 0\n",
    "        \n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHyperparameter(architecture, number_of_nets):\n",
    "    # Hyperparameter for the model (fit manually)\n",
    "    num_labels = 2 # gamma or proton\n",
    "    num_channels = 1 # it is a greyscale image\n",
    "    \n",
    "    num_steps = 20001     # Maximum batches for the model\n",
    "    \n",
    "    min_batch_size = 64   # How many images will be in a batch\n",
    "    max_batch_size = 257\n",
    "    \n",
    "    patch_size = [3, 5]   # Will the kernel/patch be 3x3 or 5x5\n",
    "\n",
    "    min_depth = 2         # Setting the depth of the convolution layers. New layers will be longer than the preceding\n",
    "    max_depth = 21\n",
    "    \n",
    "    min_num_hidden = 8    # Number of hidden nodes in f-layers. all f-layers will have the same number of nodes\n",
    "    max_num_hidden = 257\n",
    "    \n",
    "    \n",
    "    num_steps = [num_steps] * number_of_nets\n",
    "    batch_size = np.random.randint(min_batch_size, max_batch_size, size=number_of_nets)\n",
    "    patch_size = np.random.choice(patch_size, size=number_of_nets)\n",
    "    layer = architecture[:-1]\n",
    "\n",
    "    depth = []\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets)) # 2 - 21\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[0])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[1])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[2])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[3])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[4])\n",
    "\n",
    "    num_hidden = np.random.randint(min_num_hidden, max_num_hidden, size=number_of_nets)\n",
    "    \n",
    "    # Combining the hyperparameters to fit them into a for-loop\n",
    "    hyperparameter = zip(num_steps, batch_size, patch_size, zip(*depth), num_hidden)\n",
    "    \n",
    "    return num_labels, num_channels, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSessConf(per_process_gpu_memory_fraction = 0.1, op_parallelism_threads = 18):\n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=per_process_gpu_memory_fraction)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=op_parallelism_threads, inter_op_parallelism_threads=op_parallelism_threads)\n",
    "    \n",
    "    return session_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input arguments from outside\n",
    "#path_mc_images = sys.argv[1]\n",
    "#save_model_path = sys.argv[2]\n",
    "\n",
    "path_mc_images = '/fhgfs/users/jbehnken/make_Data/MC_diffuse_flat_preprocessed_images.h5'\n",
    "save_model_path = '/fhgfs/users/jbehnken/crap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(steps, best_auc):\n",
    "    print('Layer {} training:'.format(pretraining_step))\n",
    "    gen = batchYielder(path_mc_images)\n",
    "    for step in range(steps):\n",
    "        batch_data, batch_labels = next(gen)\n",
    "        # Creating a feed_dict to train the model on in this step\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        # Train the model for this step\n",
    "        _ = sess.run([optimizer], feed_dict=feed_dict)\n",
    "\n",
    "        # Updating the output to stay in touch with the training process\n",
    "        # Checking for early-stopping with scikit-learn\n",
    "        if (step % 100 == 0):\n",
    "            s = sess.run(summ, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "            # Compute the accuracy and the roc-auc-score with scikit-learn\n",
    "            pred = sess.run(valid_prediction)\n",
    "            pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "            stop_acc = accuracy_score(np.argmax(valid_labels, axis=1), np.argmax(pred, axis=1))\n",
    "            stop_auc = roc_auc_score(valid_labels, pred)\n",
    "\n",
    "            # Check if early-stopping is necessary\n",
    "            auc_now = stop_auc\n",
    "            if step == 0:\n",
    "                stopping_auc = 0.0\n",
    "                sink_count = 0\n",
    "            else:\n",
    "                if auc_now > stopping_auc:\n",
    "                    stopping_auc = auc_now\n",
    "                    sink_count = 0\n",
    "                    # Check if the model is better than the existing one and has to be saved\n",
    "                    if stopping_auc > best_auc:\n",
    "                        saver.save(sess, os.path.join(folder_path, architecture))\n",
    "                        best_auc = stopping_auc\n",
    "                else:\n",
    "                    sink_count += 1\n",
    "                    \n",
    "            # Printing a current evaluation of the model\n",
    "            print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, stop_acc*100, step))\n",
    "            if sink_count == 10:\n",
    "                break   \n",
    "\n",
    "    return stop_acc, stopping_auc, step, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(folder_path, model_name+'_Pretraining.csv'), 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Accuracy', 'Auc', 'Pretraining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " /fhgfs/users/jbehnken/crap/1_ccccccffff\n",
      "20001 111 3 (3, 16, 21, 36, 39, 50) 115\n",
      "Layers created\n",
      "Layer 1 training:\n",
      "St_auc: 0.0, sc: 0,val: 54.94, Step: 0\n",
      "St_auc: 0.47720460411075083, sc: 0,val: 51.12, Step: 100\n",
      "St_auc: 0.5060113191310183, sc: 0,val: 51.42, Step: 200\n",
      "St_auc: 0.5194550290992506, sc: 0,val: 51.94, Step: 300\n",
      "St_auc: 0.5274232908078231, sc: 0,val: 53.14, Step: 400\n",
      "St_auc: 0.5275939971475156, sc: 0,val: 53.18000000000001, Step: 500\n",
      "St_auc: 0.5275939971475156, sc: 1,val: 53.22, Step: 600\n",
      "St_auc: 0.5275939971475156, sc: 2,val: 53.300000000000004, Step: 700\n",
      "St_auc: 0.5275939971475156, sc: 3,val: 53.400000000000006, Step: 800\n",
      "St_auc: 0.5275939971475156, sc: 4,val: 53.18000000000001, Step: 900\n",
      "Layers created\n",
      "Layer 2 training:\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5011414219216024, sc: 0,val: 54.82, Step: 100\n",
      "St_auc: 0.5011414219216024, sc: 1,val: 45.64, Step: 200\n",
      "St_auc: 0.5084131242071608, sc: 0,val: 54.279999999999994, Step: 300\n",
      "St_auc: 0.5451819610013975, sc: 0,val: 55.32, Step: 400\n",
      "St_auc: 0.5451819610013975, sc: 1,val: 55.00000000000001, Step: 500\n",
      "St_auc: 0.5451819610013975, sc: 2,val: 54.92, Step: 600\n",
      "St_auc: 0.5451819610013975, sc: 3,val: 54.96, Step: 700\n",
      "St_auc: 0.5451819610013975, sc: 4,val: 54.94, Step: 800\n",
      "St_auc: 0.5451819610013975, sc: 5,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 3 training:\n",
      "St_auc: 0.0, sc: 0,val: 54.94, Step: 0\n",
      "St_auc: 0.5221592259546022, sc: 0,val: 54.96, Step: 100\n",
      "St_auc: 0.5221592259546022, sc: 1,val: 54.94, Step: 200\n",
      "St_auc: 0.5221592259546022, sc: 2,val: 54.94, Step: 300\n",
      "St_auc: 0.5221592259546022, sc: 3,val: 54.94, Step: 400\n",
      "St_auc: 0.5221592259546022, sc: 4,val: 54.94, Step: 500\n",
      "St_auc: 0.5221592259546022, sc: 5,val: 54.94, Step: 600\n",
      "St_auc: 0.5221592259546022, sc: 6,val: 54.94, Step: 700\n",
      "St_auc: 0.5221592259546022, sc: 7,val: 54.94, Step: 800\n",
      "St_auc: 0.5221592259546022, sc: 8,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 4 training:\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5001891261435022, sc: 0,val: 54.94, Step: 100\n",
      "St_auc: 0.5001891261435022, sc: 1,val: 54.94, Step: 200\n",
      "St_auc: 0.5001891261435022, sc: 2,val: 54.94, Step: 300\n",
      "St_auc: 0.5001891261435022, sc: 3,val: 54.94, Step: 400\n",
      "St_auc: 0.5059507276711179, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.5064145754938083, sc: 0,val: 54.94, Step: 600\n",
      "St_auc: 0.5064145754938083, sc: 1,val: 54.94, Step: 700\n",
      "St_auc: 0.5064145754938083, sc: 2,val: 54.94, Step: 800\n",
      "St_auc: 0.5064145754938083, sc: 3,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 5 training:\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5, sc: 0,val: 54.94, Step: 100\n",
      "St_auc: 0.5, sc: 1,val: 54.94, Step: 200\n",
      "St_auc: 0.5, sc: 2,val: 54.94, Step: 300\n",
      "St_auc: 0.5, sc: 3,val: 54.94, Step: 400\n",
      "St_auc: 0.5, sc: 4,val: 54.94, Step: 500\n",
      "St_auc: 0.5, sc: 5,val: 54.94, Step: 600\n",
      "St_auc: 0.5, sc: 6,val: 54.94, Step: 700\n",
      "St_auc: 0.5, sc: 7,val: 54.94, Step: 800\n",
      "St_auc: 0.5, sc: 8,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 6 training:\n",
      "St_auc: 0.0, sc: 0,val: 54.94, Step: 0\n",
      "St_auc: 0.5010492825082473, sc: 0,val: 54.94, Step: 100\n",
      "St_auc: 0.5149485998606236, sc: 0,val: 54.94, Step: 200\n",
      "St_auc: 0.5149485998606236, sc: 1,val: 54.94, Step: 300\n",
      "St_auc: 0.5149485998606236, sc: 2,val: 54.94, Step: 400\n",
      "St_auc: 0.5149485998606236, sc: 3,val: 54.94, Step: 500\n",
      "St_auc: 0.5149485998606236, sc: 4,val: 54.94, Step: 600\n",
      "St_auc: 0.5159951759503285, sc: 0,val: 54.94, Step: 700\n",
      "St_auc: 0.5170287046790019, sc: 0,val: 54.94, Step: 800\n",
      "St_auc: 0.5246463842005911, sc: 0,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 7 training:\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5323097900772517, sc: 0,val: 54.94, Step: 100\n",
      "St_auc: 0.5323097900772517, sc: 1,val: 54.94, Step: 200\n",
      "St_auc: 0.5323097900772517, sc: 2,val: 54.94, Step: 300\n",
      "St_auc: 0.5323097900772517, sc: 3,val: 54.94, Step: 400\n",
      "St_auc: 0.5399687525801863, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.5399687525801863, sc: 1,val: 54.94, Step: 600\n",
      "St_auc: 0.5399687525801863, sc: 2,val: 54.94, Step: 700\n",
      "St_auc: 0.5399687525801863, sc: 3,val: 54.94, Step: 800\n",
      "St_auc: 0.5399687525801863, sc: 4,val: 54.94, Step: 900\n",
      "Layers created\n",
      "Layer 8 training:\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5056710778865245, sc: 0,val: 54.94, Step: 100\n",
      "St_auc: 0.5056710778865245, sc: 1,val: 54.94, Step: 200\n",
      "St_auc: 0.510161914922804, sc: 0,val: 54.94, Step: 300\n",
      "St_auc: 0.510161914922804, sc: 1,val: 54.94, Step: 400\n",
      "St_auc: 0.514780317179327, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.514780317179327, sc: 1,val: 54.94, Step: 600\n",
      "St_auc: 0.514780317179327, sc: 2,val: 54.94, Step: 700\n",
      "St_auc: 0.514780317179327, sc: 3,val: 54.94, Step: 800\n",
      "St_auc: 0.514780317179327, sc: 4,val: 54.94, Step: 900\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 50 and 115 for '9_fc_2/MatMul' (op: 'MatMul') with input shapes: [111,50], [115,115].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 50 and 115 for '9_fc_2/MatMul' (op: 'MatMul') with input shapes: [111,50], [115,115].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-130d8864cdc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1041\u001b[0m                         \u001b[0minit_b_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases_8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                         \u001b[0mlayer8_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B_36'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_b_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer8_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer8_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m                         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1765\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   \"\"\"\n\u001b[1;32m   1453\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1454\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1455\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 50 and 115 for '9_fc_2/MatMul' (op: 'MatMul') with input shapes: [111,50], [115,115]."
     ]
    }
   ],
   "source": [
    "dropout_rate_c = 0.9\n",
    "dropout_rate_c_output = 0.75\n",
    "dropout_rate_f = 0.5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "pretraining_steps = [1001, 2001, 5001, 8001]\n",
    "\n",
    "# Number of events in training-dataset\n",
    "num_events = 500000\n",
    "\n",
    "# Number of events in validation-/test-dataset\n",
    "events_in_validation_and_testing = 5000\n",
    "\n",
    "# Number of nets to compute\n",
    "number_of_nets = 30\n",
    "\n",
    "trainable = True\n",
    "\n",
    "# Architectures to test\n",
    "test_architectures = ['ccccccffff']\n",
    "\n",
    "\n",
    "\n",
    "gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder(path_mc_images)\n",
    "valid_dataset, valid_labels, test_dataset, test_labels = getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count)\n",
    "\n",
    "\n",
    "for architecture in test_architectures:\n",
    "    c_count = architecture.count('c')\n",
    "    f_count = architecture.count('f')\n",
    "    folder_path = createFolders(architecture, save_model_path)\n",
    "    print('\\n\\n', folder_path)\n",
    "    \n",
    "    best_auc = bestAuc(folder_path, architecture)\n",
    "        \n",
    "    num_labels, num_channels, hyperparameter = getHyperparameter(architecture, number_of_nets)\n",
    "    for pretraining in pretraining_steps:\n",
    "        for num_steps, batch_size, patch_size, depth, num_hidden in hyperparameter:\n",
    "            try:\n",
    "                print(num_steps, batch_size, patch_size, depth, num_hidden)\n",
    "\n",
    "                # Measuring the loop-time\n",
    "                start = time.time()\n",
    "                # Path to logfiles and correct file name\n",
    "                LOGDIR = '/fhgfs/users/jbehnken/tf_logs/small_logs'\n",
    "                # Getting the right count-number for the new logfiles\n",
    "                logcount = str(len(os.listdir(LOGDIR)))\n",
    "                hparams = '_bs={}_ps={}_d={}_nh={}_ns={}'.format(batch_size, patch_size, depth, num_hidden, num_steps)\n",
    "\n",
    "\n",
    "                tf.reset_default_graph()\n",
    "                with tf.Session(config=getSessConf()) as sess:\n",
    "                    # Create tf.variables for the three different datasets\n",
    "                    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "                    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "                    tf_valid_dataset = tf.constant(valid_dataset, name='valid_data')\n",
    "                    tf_valid_labels = tf.constant(valid_labels, name='valid_labels')\n",
    "\n",
    "                    tf_test_dataset_final = tf.constant(test_dataset, name='test_data_final')\n",
    "                    tf_test_labels_final = tf.constant(test_labels, name='test_labels_final')                    \n",
    "\n",
    "                    # Summary for same example input images\n",
    "                    tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "\n",
    "                        \n",
    "                    weights_1 = []\n",
    "                    biases_1 = []\n",
    "                    pretraining_step = 1\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth[0]], stddev=0.1), name='W_1')\n",
    "                        layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth[0]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([23*23*depth[0], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_1.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_1, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'First_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_1.append(layer1_weights.eval())\n",
    "                    biases_1.append(layer1_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_2 = []\n",
    "                    biases_2 = []\n",
    "                    pretraining_step = 2\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_1[0])\n",
    "                        layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_1[0])\n",
    "                        layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[0], depth[1]], stddev=0.1), name='W_1')\n",
    "                        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth[1]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([12*12*depth[1], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_2.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Second_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_2.append(layer1_weights.eval())\n",
    "                    weights_2.append(layer2_weights.eval())\n",
    "                    biases_2.append(layer1_biases.eval())\n",
    "                    biases_2.append(layer2_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_3 = []\n",
    "                    biases_3 = []\n",
    "                    pretraining_step = 3\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_2[0])\n",
    "                        layer1_weights = tf.get_variable('W_2', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_2[0])\n",
    "                        layer1_biases = tf.get_variable('B_2', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_2[1])\n",
    "                        layer2_weights = tf.get_variable('W_3', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_2[1])\n",
    "                        layer2_biases = tf.get_variable('B_3', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[1], depth[2]], stddev=0.1), name='W_1')\n",
    "                        layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth[2]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([6*6*depth[2], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_3.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Third_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_3.append(layer1_weights.eval())\n",
    "                    weights_3.append(layer2_weights.eval())\n",
    "                    weights_3.append(layer3_weights.eval())\n",
    "                    biases_3.append(layer1_biases.eval())\n",
    "                    biases_3.append(layer2_biases.eval())\n",
    "                    biases_3.append(layer3_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_4 = []\n",
    "                    biases_4 = []\n",
    "                    pretraining_step = 4\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_3[0])\n",
    "                        layer1_weights = tf.get_variable('W_4', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_3[0])\n",
    "                        layer1_biases = tf.get_variable('B_4', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_3[1])\n",
    "                        layer2_weights = tf.get_variable('W_5', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_3[1])\n",
    "                        layer2_biases = tf.get_variable('B_5', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_3[2])\n",
    "                        layer3_weights = tf.get_variable('W_6', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_3[2])\n",
    "                        layer3_biases = tf.get_variable('B_6', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[2], depth[3]], stddev=0.1), name='W_1')\n",
    "                        layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth[3]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([3*3*depth[3], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_4.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Fourth_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_4.append(layer1_weights.eval())\n",
    "                    weights_4.append(layer2_weights.eval())\n",
    "                    weights_4.append(layer3_weights.eval())\n",
    "                    weights_4.append(layer4_weights.eval())\n",
    "                    biases_4.append(layer1_biases.eval())\n",
    "                    biases_4.append(layer2_biases.eval())\n",
    "                    biases_4.append(layer3_biases.eval())\n",
    "                    biases_4.append(layer4_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_5 = []\n",
    "                    biases_5 = []\n",
    "                    pretraining_step = 5\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_4[0])\n",
    "                        layer1_weights = tf.get_variable('W_7', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_4[0])\n",
    "                        layer1_biases = tf.get_variable('B_7', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_4[1])\n",
    "                        layer2_weights = tf.get_variable('W_8', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_4[1])\n",
    "                        layer2_biases = tf.get_variable('B_8', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_4[2])\n",
    "                        layer3_weights = tf.get_variable('W_9', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_4[2])\n",
    "                        layer3_biases = tf.get_variable('B_9', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        init_w_4 = tf.constant(weights_4[3])\n",
    "                        layer4_weights = tf.get_variable('W_10', initializer=init_w_4, trainable=trainable)\n",
    "                        init_b_4 = tf.constant(biases_4[3])\n",
    "                        layer4_biases = tf.get_variable('B_10', initializer=init_b_4, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_5'.format(pretraining_step)):\n",
    "                        layer5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[3], depth[4]], stddev=0.1), name='W_1')\n",
    "                        layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth[4]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([2*2*depth[4], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_5.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_5, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Fifth_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_5.append(layer1_weights.eval())\n",
    "                    weights_5.append(layer2_weights.eval())\n",
    "                    weights_5.append(layer3_weights.eval())\n",
    "                    weights_5.append(layer4_weights.eval())\n",
    "                    weights_5.append(layer5_weights.eval())\n",
    "                    biases_5.append(layer1_biases.eval())\n",
    "                    biases_5.append(layer2_biases.eval())\n",
    "                    biases_5.append(layer3_biases.eval())\n",
    "                    biases_5.append(layer4_biases.eval())\n",
    "                    biases_5.append(layer5_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_6 = []\n",
    "                    biases_6 = []\n",
    "                    pretraining_step = 6\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_5[0])\n",
    "                        layer1_weights = tf.get_variable('W_11', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_5[0])\n",
    "                        layer1_biases = tf.get_variable('B_11', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_5[1])\n",
    "                        layer2_weights = tf.get_variable('W_12', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_5[1])\n",
    "                        layer2_biases = tf.get_variable('B_12', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_5[2])\n",
    "                        layer3_weights = tf.get_variable('W_13', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_5[2])\n",
    "                        layer3_biases = tf.get_variable('B_13', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        init_w_4 = tf.constant(weights_5[3])\n",
    "                        layer4_weights = tf.get_variable('W_14', initializer=init_w_4, trainable=trainable)\n",
    "                        init_b_4 = tf.constant(biases_5[3])\n",
    "                        layer4_biases = tf.get_variable('B_14', initializer=init_b_4, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_5'.format(pretraining_step)):\n",
    "                        init_w_5 = tf.constant(weights_5[4])\n",
    "                        layer5_weights = tf.get_variable('W_15', initializer=init_w_5, trainable=trainable)\n",
    "                        init_b_5 = tf.constant(biases_5[4])\n",
    "                        layer5_biases = tf.get_variable('B_15', initializer=init_b_5, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_6'.format(pretraining_step)):\n",
    "                        layer6_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[4], depth[5]], stddev=0.1), name='W_1')\n",
    "                        layer6_biases = tf.Variable(tf.constant(1.0, shape=[depth[5]]), name='B_1')\n",
    "                        conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([1*1*depth[5], num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_6 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_5, layer6_weights, [1, 1, 1, 1], padding='SAME') + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_6.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_6, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Sixth_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_6.append(layer1_weights.eval())\n",
    "                    weights_6.append(layer2_weights.eval())\n",
    "                    weights_6.append(layer3_weights.eval())\n",
    "                    weights_6.append(layer4_weights.eval())\n",
    "                    weights_6.append(layer5_weights.eval())\n",
    "                    weights_6.append(layer6_weights.eval())\n",
    "                    biases_6.append(layer1_biases.eval())\n",
    "                    biases_6.append(layer2_biases.eval())\n",
    "                    biases_6.append(layer3_biases.eval())\n",
    "                    biases_6.append(layer4_biases.eval())\n",
    "                    biases_6.append(layer5_biases.eval())\n",
    "                    biases_6.append(layer6_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_7 = []\n",
    "                    biases_7 = []\n",
    "                    pretraining_step = 7\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_6[0])\n",
    "                        layer1_weights = tf.get_variable('W_16', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_6[0])\n",
    "                        layer1_biases = tf.get_variable('B_16', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_6[1])\n",
    "                        layer2_weights = tf.get_variable('W_17', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_6[1])\n",
    "                        layer2_biases = tf.get_variable('B_17', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_6[2])\n",
    "                        layer3_weights = tf.get_variable('W_18', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_6[2])\n",
    "                        layer3_biases = tf.get_variable('B_18', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        init_w_4 = tf.constant(weights_6[3])\n",
    "                        layer4_weights = tf.get_variable('W_19', initializer=init_w_4, trainable=trainable)\n",
    "                        init_b_4 = tf.constant(biases_6[3])\n",
    "                        layer4_biases = tf.get_variable('B_19', initializer=init_b_4, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_5'.format(pretraining_step)):\n",
    "                        init_w_5 = tf.constant(weights_6[4])\n",
    "                        layer5_weights = tf.get_variable('W_20', initializer=init_w_5, trainable=trainable)\n",
    "                        init_b_5 = tf.constant(biases_6[4])\n",
    "                        layer5_biases = tf.get_variable('B_20', initializer=init_b_5, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_6'.format(pretraining_step)):\n",
    "                        init_w_6 = tf.constant(weights_6[5])\n",
    "                        layer6_weights = tf.get_variable('W_21', initializer=init_w_6, trainable=trainable)\n",
    "                        init_b_6 = tf.constant(biases_6[5])\n",
    "                        layer6_biases = tf.get_variable('B_21', initializer=init_b_6, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_1'.format(pretraining_step)):\n",
    "                        layer7_weights = tf.Variable(tf.truncated_normal([1*1*depth[5], num_hidden], stddev=0.1), name='W')\n",
    "                        layer7_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "                        hidden = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_6 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_5, layer6_weights, [1, 1, 1, 1], padding='SAME') + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_6.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_6, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        hidden_1 = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(hidden_1, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Seventh_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_7.append(layer1_weights.eval())\n",
    "                    weights_7.append(layer2_weights.eval())\n",
    "                    weights_7.append(layer3_weights.eval())\n",
    "                    weights_7.append(layer4_weights.eval())\n",
    "                    weights_7.append(layer5_weights.eval())\n",
    "                    weights_7.append(layer6_weights.eval())\n",
    "                    weights_7.append(layer7_weights.eval())\n",
    "                    biases_7.append(layer1_biases.eval())\n",
    "                    biases_7.append(layer2_biases.eval())\n",
    "                    biases_7.append(layer3_biases.eval())\n",
    "                    biases_7.append(layer4_biases.eval())\n",
    "                    biases_7.append(layer5_biases.eval())\n",
    "                    biases_7.append(layer6_biases.eval())\n",
    "                    biases_7.append(layer7_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_8 = []\n",
    "                    biases_8 = []\n",
    "                    pretraining_step = 8\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_7[0])\n",
    "                        layer1_weights = tf.get_variable('W_22', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_7[0])\n",
    "                        layer1_biases = tf.get_variable('B_22', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_7[1])\n",
    "                        layer2_weights = tf.get_variable('W_23', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_7[1])\n",
    "                        layer2_biases = tf.get_variable('B_23', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_7[2])\n",
    "                        layer3_weights = tf.get_variable('W_24', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_7[2])\n",
    "                        layer3_biases = tf.get_variable('B_24', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        init_w_4 = tf.constant(weights_7[3])\n",
    "                        layer4_weights = tf.get_variable('W_25', initializer=init_w_4, trainable=trainable)\n",
    "                        init_b_4 = tf.constant(biases_7[3])\n",
    "                        layer4_biases = tf.get_variable('B_25', initializer=init_b_4, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_5'.format(pretraining_step)):\n",
    "                        init_w_5 = tf.constant(weights_7[4])\n",
    "                        layer5_weights = tf.get_variable('W_26', initializer=init_w_5, trainable=trainable)\n",
    "                        init_b_5 = tf.constant(biases_7[4])\n",
    "                        layer5_biases = tf.get_variable('B_26', initializer=init_b_5, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_6'.format(pretraining_step)):\n",
    "                        init_w_6 = tf.constant(weights_7[5])\n",
    "                        layer6_weights = tf.get_variable('W_27', initializer=init_w_6, trainable=trainable)\n",
    "                        init_b_6 = tf.constant(biases_7[5])\n",
    "                        layer6_biases = tf.get_variable('B_27', initializer=init_b_6, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_1'.format(pretraining_step)):\n",
    "                        init_w_7 = tf.constant(weights_7[6])\n",
    "                        layer7_weights = tf.get_variable('W_28', initializer=init_w_7, trainable=trainable)\n",
    "                        init_b_7 = tf.constant(biases_7[6])\n",
    "                        layer7_biases = tf.get_variable('B_28', initializer=init_b_7, trainable=trainable)\n",
    "                        hidden = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_2'.format(pretraining_step)):\n",
    "                        layer8_weights = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev=0.1), name='W')\n",
    "                        layer8_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "                        hidden = tf.nn.relu(tf.matmul(hidden, layer8_weights) + layer8_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_6 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_5, layer6_weights, [1, 1, 1, 1], padding='SAME') + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_6.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_6, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        hidden_1 = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer8_weights) + layer8_biases)\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(hidden_2, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Seventh_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(pretraining, best_auc)\n",
    "\n",
    "                    weights_8.append(layer1_weights.eval())\n",
    "                    weights_8.append(layer2_weights.eval())\n",
    "                    weights_8.append(layer3_weights.eval())\n",
    "                    weights_8.append(layer4_weights.eval())\n",
    "                    weights_8.append(layer5_weights.eval())\n",
    "                    weights_8.append(layer6_weights.eval())\n",
    "                    weights_8.append(layer7_weights.eval())\n",
    "                    weights_8.append(layer8_weights.eval())\n",
    "                    biases_8.append(layer1_biases.eval())\n",
    "                    biases_8.append(layer2_biases.eval())\n",
    "                    biases_8.append(layer3_biases.eval())\n",
    "                    biases_8.append(layer4_biases.eval())\n",
    "                    biases_8.append(layer5_biases.eval())\n",
    "                    biases_8.append(layer6_biases.eval())\n",
    "                    biases_8.append(layer7_biases.eval())\n",
    "                    biases_8.append(layer8_biases.eval())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    weights_9 = []\n",
    "                    biases_9 = []\n",
    "                    pretraining_step = 9\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_1'.format(pretraining_step)):\n",
    "                        init_w_1 = tf.constant(weights_8[0])\n",
    "                        layer1_weights = tf.get_variable('W_29', initializer=init_w_1, trainable=trainable)\n",
    "                        init_b_1 = tf.constant(biases_8[0])\n",
    "                        layer1_biases = tf.get_variable('B_29', initializer=init_b_1, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_2'.format(pretraining_step)):\n",
    "                        init_w_2 = tf.constant(weights_8[1])\n",
    "                        layer2_weights = tf.get_variable('W_30', initializer=init_w_2, trainable=trainable)\n",
    "                        init_b_2 = tf.constant(biases_8[1])\n",
    "                        layer2_biases = tf.get_variable('B_30', initializer=init_b_2, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_3'.format(pretraining_step)):\n",
    "                        init_w_3 = tf.constant(weights_8[2])\n",
    "                        layer3_weights = tf.get_variable('W_31', initializer=init_w_3, trainable=trainable)\n",
    "                        init_b_3 = tf.constant(biases_8[2])\n",
    "                        layer3_biases = tf.get_variable('B_31', initializer=init_b_3, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_4'.format(pretraining_step)):\n",
    "                        init_w_4 = tf.constant(weights_8[3])\n",
    "                        layer4_weights = tf.get_variable('W_32', initializer=init_w_4, trainable=trainable)\n",
    "                        init_b_4 = tf.constant(biases_8[3])\n",
    "                        layer4_biases = tf.get_variable('B_32', initializer=init_b_4, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_5'.format(pretraining_step)):\n",
    "                        init_w_5 = tf.constant(weights_8[4])\n",
    "                        layer5_weights = tf.get_variable('W_33', initializer=init_w_5, trainable=trainable)\n",
    "                        init_b_5 = tf.constant(biases_8[4])\n",
    "                        layer5_biases = tf.get_variable('B_33', initializer=init_b_5, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "                        \n",
    "                    with tf.name_scope('{}_conv2d_6'.format(pretraining_step)):\n",
    "                        init_w_6 = tf.constant(weights_8[5])\n",
    "                        layer6_weights = tf.get_variable('W_34', initializer=init_w_6, trainable=trainable)\n",
    "                        init_b_6 = tf.constant(biases_8[5])\n",
    "                        layer6_biases = tf.get_variable('B_34', initializer=init_b_6, trainable=trainable)\n",
    "                        conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "                    # The reshape produces an input vector for the dense layer\n",
    "                    with tf.name_scope('{}_reshape'.format(pretraining_step)):\n",
    "                        shape = pool.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_1'.format(pretraining_step)):\n",
    "                        init_w_7 = tf.constant(weights_8[6])\n",
    "                        layer7_weights = tf.get_variable('W_35', initializer=init_w_7, trainable=trainable)\n",
    "                        init_b_7 = tf.constant(biases_8[6])\n",
    "                        layer7_biases = tf.get_variable('B_35', initializer=init_b_7, trainable=trainable)\n",
    "                        hidden = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_2'.format(pretraining_step)):\n",
    "                        init_w_8 = tf.constant(weights_8[7])\n",
    "                        layer8_weights = tf.get_variable('W_36', initializer=init_w_8, trainable=trainable)\n",
    "                        init_b_8 = tf.constant(biases_8[7])\n",
    "                        layer8_biases = tf.get_variable('B_36', initializer=init_b_8, trainable=trainable)\n",
    "                        hidden = tf.nn.relu(tf.matmul(hidden, layer8_weights) + layer8_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "                        \n",
    "                    with tf.name_scope('{}_fc_3'.format(pretraining_step)):\n",
    "                        layer9_weights = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev=0.1), name='W')\n",
    "                        layer9_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "                        hidden = tf.nn.relu(tf.matmul(hidden, layer9_weights) + layer9_biases)\n",
    "                        hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "                    # Output layer is a dense layer\n",
    "                    with tf.name_scope('{}_Output'.format(pretraining_step)):\n",
    "                        output_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name='W')\n",
    "                        output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "                        output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "                    # Computing the loss of the model\n",
    "                    with tf.name_scope('{}_loss'.format(pretraining_step)):\n",
    "                        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "\n",
    "                    # Optimizing the model\n",
    "                    with tf.name_scope('{}_optimizer'.format(pretraining_step)):\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(pretraining_step)).minimize(loss)\n",
    "\n",
    "                    # Predictions for the training, validation, and test data\n",
    "                    with tf.name_scope('{}_prediction'.format(pretraining_step)):\n",
    "                        train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_6 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_5, layer6_weights, [1, 1, 1, 1], padding='SAME') + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_6.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_6, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        hidden_1 = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer8_weights) + layer8_biases)\n",
    "                        hidden_3 = tf.nn.relu(tf.matmul(hidden_2, layer9_weights) + layer9_biases)\n",
    "                        valid_prediction = tf.nn.softmax(tf.matmul(hidden_3, output_weights) + output_biases)\n",
    "\n",
    "                        correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(tf_valid_labels, 1))\n",
    "                        valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "                    \n",
    "                    # Evaluating the network: accuracy\n",
    "                    with tf.name_scope('{}_valid'.format(pretraining_step)):\n",
    "                        pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset_final, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_4 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_3, layer4_weights, [1, 1, 1, 1], padding='SAME') + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_5 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_4, layer5_weights, [1, 1, 1, 1], padding='SAME') + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool_6 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_5, layer6_weights, [1, 1, 1, 1], padding='SAME') + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        shape = pool_6.get_shape().as_list()\n",
    "                        reshape = tf.reshape(pool_6, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        hidden_1 = tf.nn.relu(tf.matmul(reshape, layer7_weights) + layer7_biases)\n",
    "                        hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer8_weights) + layer8_biases)\n",
    "                        hidden_3 = tf.nn.relu(tf.matmul(hidden_2, layer9_weights) + layer9_biases)\n",
    "                        test_prediction = tf.nn.softmax(tf.matmul(hidden_3, output_weights) + output_biases)\n",
    "\n",
    "                        test_correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(tf_test_labels_final, 1))\n",
    "                        test_accuracy = tf.reduce_mean(tf.cast(test_correct_prediction, tf.float32))                 \n",
    "                        \n",
    "                    # Evaluating the network: auc\n",
    "                    with tf.name_scope('{}_auc'.format(pretraining_step)):\n",
    "                        test_auc = tf.metrics.auc(labels=tf_test_labels_final, predictions=test_prediction, curve='ROC')\n",
    "                    print('Layers created')\n",
    "\n",
    "\n",
    "                    summ = tf.summary.merge_all()\n",
    "                    saver = tf.train.Saver()\n",
    "\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    writer = tf.summary.FileWriter(os.path.join(save_model_path, 'Nineth_Layer'+hparams))\n",
    "                    writer.add_graph(sess.graph)\n",
    "\n",
    "                    stop_acc, stopping_auc, step, best_auc = training(num_steps, best_auc)\n",
    "                    \n",
    "                    pred = sess.run(test_prediction)\n",
    "                    pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "                    stop_acc = accuracy_score(np.argmax(tf_test_labels_final, axis=1), np.argmax(pred, axis=1))\n",
    "                    stop_auc = roc_auc_score(tf_test_labels_final, pred)\n",
    "                    \n",
    "                    with open(os.path.join(folder_path, model_name+'_Pretraining.csv'), 'a') as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerow([stop_acc, stop_auc, pretraining])\n",
    "\n",
    "                    weights_9.append(layer1_weights.eval())\n",
    "                    weights_9.append(layer2_weights.eval())\n",
    "                    weights_9.append(layer3_weights.eval())\n",
    "                    weights_9.append(layer4_weights.eval())\n",
    "                    weights_9.append(layer5_weights.eval())\n",
    "                    weights_9.append(layer6_weights.eval())\n",
    "                    weights_9.append(layer7_weights.eval())\n",
    "                    weights_9.append(layer8_weights.eval())\n",
    "                    weights_9.append(layer9_weights.eval())\n",
    "                    biases_9.append(layer1_biases.eval())\n",
    "                    biases_9.append(layer2_biases.eval())\n",
    "                    biases_9.append(layer3_biases.eval())\n",
    "                    biases_9.append(layer4_biases.eval())\n",
    "                    biases_9.append(layer5_biases.eval())\n",
    "                    biases_9.append(layer6_biases.eval())\n",
    "                    biases_9.append(layer7_biases.eval())\n",
    "                    biases_9.append(layer8_biases.eval())\n",
    "                    biases_9.append(layer9_biases.eval())\n",
    "                    \n",
    "        \n",
    "        \n",
    "            except:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2 in zip(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2):\n",
    "    hparams = '_bs={}_ps={}_d1={}_d2={}_d3={}_nh1={}_nh2={}'.format(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2)\n",
    "    run_path, folder_path, count = createFolderstructure()\n",
    "        \n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.2)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=18, inter_op_parallelism_threads=18)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    iteration = 5\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session created')\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_4[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_4[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_4[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_4[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            init_w_3 = tf.constant(weights_4[2])\n",
    "            layer3_weights = tf.get_variable('W_3', initializer=init_w_3, trainable=trainable)\n",
    "            init_b_3 = tf.constant(biases_4[2])\n",
    "            layer3_biases = tf.get_variable('B_3', initializer=init_b_3, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Fourth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_1'.format(iteration)):\n",
    "            init_w_4 = tf.constant(weights_4[3])\n",
    "            layer4_weights = tf.get_variable('W_4', initializer=init_w_4, trainable=trainable)\n",
    "            init_b_4 = tf.constant(biases_4[3])\n",
    "            layer4_biases = tf.get_variable('B_4', initializer=init_b_4, trainable=trainable)\n",
    "            \n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Fifth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_2'.format(iteration)):\n",
    "            layer5_weights = tf.Variable(tf.truncated_normal([num_hidden_f1, num_hidden_f2], stddev=0.1), name='W')\n",
    "            layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_f2]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer5_weights)\n",
    "            tf.summary.histogram(\"biases\", layer5_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([num_hidden_f2, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_1 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer5_weights) + layer5_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(hidden_2, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Fifth_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "        acc, stopping_auc, step = training(num_steps_final)\n",
    "\n",
    "        dauer = time.time() - start\n",
    "        early_stopped = True if step < num_steps_final-1 else False\n",
    "        \n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([learning_rate, batch_size, patch_size, [depth_c1, depth_c2, depth_c3], [num_hidden_f1, num_hidden_f2], acc*100, stopping_auc, step, early_stopped, dauer, str(max(count)+1)+'_'+title_name])\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
