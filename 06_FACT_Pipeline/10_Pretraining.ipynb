{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mc_data_path = '/fhgfs/users/jbehnken/01_Data/01b_MC_Data_flat' # Path to preprocessed data\n",
    "num_files = 500 # Number of files to load - 1 file = 1000 events\n",
    "events_in_validation = 10000\n",
    "number_of_nets = 1\n",
    "\n",
    "save_model_path = '/fhgfs/users/jbehnken/01_Data/04_Models'\n",
    "model_name = 'pre-cccfff'\n",
    "title_name = 'with_5_100_pre_flat'\n",
    "\n",
    "# Hyperparameter for the model (fit manually)\n",
    "num_labels = 2 # gamma or proton\n",
    "num_channels = 1 # it is a greyscale image\n",
    "\n",
    "num_steps_pretraining = 5000\n",
    "num_steps_final = 100000\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = np.random.randint(64, 257, size=number_of_nets)\n",
    "patch_size = np.random.randint(0, 1, size=number_of_nets)*2 +3\n",
    "\n",
    "depth_c1 = np.random.randint(4, 12, size=number_of_nets)\n",
    "depth_c2 = np.random.randint(4, 12, size=number_of_nets)+depth_c1\n",
    "depth_c3 = np.random.randint(4, 12, size=number_of_nets)+depth_c2\n",
    "\n",
    "num_hidden_f1 = np.random.randint(10, 60, size=number_of_nets)\n",
    "num_hidden_f2 = num_hidden_f1\n",
    "\n",
    "dropout_rate_c = 0.9\n",
    "dropout_rate_c_output = 0.75\n",
    "dropout_rate_f = 0.5\n",
    "\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(mc_data_path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Randomizing the files to load\n",
    "loading_files = os.listdir(mc_data_path)\n",
    "np.random.shuffle(loading_files)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, loading_files[:num_files])\n",
    "pics, labs = zip(*data)\n",
    "\n",
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "\n",
    "test_dataset = pic[p][:events_in_validation]\n",
    "test_labels = lab[p][:events_in_validation]\n",
    "train_dataset = pic[p][events_in_validation:]\n",
    "train_labels = lab[p][events_in_validation:]\n",
    "del p, pic, lab\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createFolderstructure():\n",
    "    # Creating folder structure\n",
    "    file_paths = os.listdir(save_model_path)\n",
    "    for path in file_paths:\n",
    "        name = '_' + model_name\n",
    "        if path.endswith(name):\n",
    "            correct_path = path\n",
    "\n",
    "    if 'correct_path' in locals():\n",
    "        folder_path = os.path.join(save_model_path, correct_path)\n",
    "    else:\n",
    "        folder_number = len(os.listdir(save_model_path))+1\n",
    "        folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Learning_Rate','Batch_Size','Patch_Size','Depth','Hidden_Nodes','Accuracy','Auc','Steps', 'Early_Stopped','Time', 'Title'])\n",
    "\n",
    "    models_path = os.path.join(folder_path, 'models_folder')\n",
    "    if not os.path.exists(models_path):\n",
    "        os.mkdir(models_path)\n",
    "\n",
    "    run_folders = os.listdir(models_path)\n",
    "    if len(run_folders)==0:\n",
    "        count = [0]\n",
    "    else:\n",
    "        count = [int(folder.split('_')[0]) for folder in run_folders]\n",
    "\n",
    "    run_path = os.path.join(models_path, str(max(count)+1)+'_'+title_name)\n",
    "    os.mkdir(run_path)\n",
    "    return run_path, folder_path, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def training(steps):\n",
    "    print('Layer {} training:'.format(iteration))\n",
    "    for step in range(steps+1):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Creating a feed_dict to train the model on in this step\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\n",
    "        opt = sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        # Updating the output to stay in touch with the training process\n",
    "        if (step % 1000 == 0):\n",
    "            [acc, auc, s] = sess.run([test_accuracy, test_auc, summ], feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "            auc_now = auc[1]                        \n",
    "            if step == 0:\n",
    "                stopping_auc = 0.0\n",
    "                sink_count = 0\n",
    "            else:\n",
    "                if auc_now > stopping_auc:\n",
    "                    stopping_auc = auc_now\n",
    "                    sink_count = 0\n",
    "                    saver.save(sess, os.path.join(run_path, 'First_Layer'))\n",
    "                else:\n",
    "                    sink_count += 1\n",
    "            print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, acc*100, step))\n",
    "            if sink_count == 5:\n",
    "                break\n",
    "    return acc, stopping_auc, step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 created\n",
      "Layers created\n",
      "Layer 1 training:\n",
      "St_auc: 0.0, sc: 0,val: 77.79000401496887, Step: 0\n",
      "St_auc: 0.7998446226119995, sc: 0,val: 79.98000383377075, Step: 1000\n",
      "St_auc: 0.8202066421508789, sc: 0,val: 82.63000249862671, Step: 2000\n",
      "St_auc: 0.8359696865081787, sc: 0,val: 82.84000158309937, Step: 3000\n",
      "St_auc: 0.8480862379074097, sc: 0,val: 82.91000723838806, Step: 4000\n",
      "St_auc: 0.8568362593650818, sc: 0,val: 82.84000158309937, Step: 5000\n",
      "Session 2 created\n",
      "Layers created\n",
      "Layer 2 training:\n",
      "St_auc: 0.0, sc: 0,val: 78.41000556945801, Step: 0\n",
      "St_auc: 0.8135004639625549, sc: 0,val: 82.88000226020813, Step: 1000\n",
      "St_auc: 0.8437936902046204, sc: 0,val: 86.62000298500061, Step: 2000\n",
      "St_auc: 0.863693356513977, sc: 0,val: 87.28001117706299, Step: 3000\n",
      "St_auc: 0.8881263732910156, sc: 0,val: 88.39000463485718, Step: 5000\n",
      "Session 3 created\n",
      "Layers created\n",
      "Layer 3 training:\n",
      "St_auc: 0.0, sc: 0,val: 78.36000919342041, Step: 0\n",
      "St_auc: 0.8826547861099243, sc: 0,val: 88.89000415802002, Step: 1000\n",
      "St_auc: 0.9138047099113464, sc: 0,val: 89.85000252723694, Step: 2000\n",
      "St_auc: 0.9287413358688354, sc: 0,val: 90.43001532554626, Step: 3000\n",
      "St_auc: 0.9372224807739258, sc: 0,val: 91.44001007080078, Step: 4000\n",
      "St_auc: 0.9426931142807007, sc: 0,val: 90.89000225067139, Step: 5000\n",
      "Session created\n",
      "Layers created\n",
      "Layer 4 training:\n",
      "St_auc: 0.0, sc: 0,val: 62.199991941452026, Step: 0\n",
      "St_auc: 0.865913450717926, sc: 0,val: 91.44001007080078, Step: 1000\n",
      "St_auc: 0.9133604168891907, sc: 0,val: 92.18001365661621, Step: 2000\n",
      "St_auc: 0.9327640533447266, sc: 0,val: 92.19000935554504, Step: 3000\n",
      "St_auc: 0.9434686899185181, sc: 0,val: 92.64000654220581, Step: 4000\n",
      "St_auc: 0.9499233961105347, sc: 0,val: 92.80000925064087, Step: 5000\n",
      "Session created\n",
      "Layers created\n",
      "Layer 5 training:\n",
      "St_auc: 0.0, sc: 0,val: 78.35999727249146, Step: 0\n",
      "St_auc: 0.9042192101478577, sc: 0,val: 92.60001182556152, Step: 1000\n",
      "St_auc: 0.9330720901489258, sc: 0,val: 92.64000654220581, Step: 2000\n",
      "St_auc: 0.9451966285705566, sc: 0,val: 92.66000986099243, Step: 3000\n",
      "St_auc: 0.9524001479148865, sc: 0,val: 92.77001023292542, Step: 4000\n",
      "St_auc: 0.9564344882965088, sc: 0,val: 92.9900050163269, Step: 5000\n",
      "St_auc: 0.9596707224845886, sc: 0,val: 93.02000999450684, Step: 6000\n",
      "St_auc: 0.9619659185409546, sc: 0,val: 93.08000802993774, Step: 7000\n",
      "St_auc: 0.9637187123298645, sc: 0,val: 92.86000728607178, Step: 8000\n",
      "St_auc: 0.9651030898094177, sc: 0,val: 92.74000525474548, Step: 9000\n",
      "St_auc: 0.966278076171875, sc: 0,val: 93.02000999450684, Step: 10000\n",
      "St_auc: 0.9672019481658936, sc: 0,val: 92.72001385688782, Step: 11000\n",
      "St_auc: 0.9680418372154236, sc: 0,val: 92.95001029968262, Step: 12000\n",
      "St_auc: 0.9687572717666626, sc: 0,val: 93.12001466751099, Step: 13000\n",
      "St_auc: 0.9693670272827148, sc: 0,val: 93.13001036643982, Step: 14000\n",
      "St_auc: 0.9698842167854309, sc: 0,val: 92.91000366210938, Step: 15000\n",
      "St_auc: 0.9703060388565063, sc: 0,val: 92.83000826835632, Step: 16000\n",
      "St_auc: 0.9706621766090393, sc: 0,val: 92.7600085735321, Step: 17000\n",
      "St_auc: 0.9710131883621216, sc: 0,val: 93.03001165390015, Step: 18000\n",
      "St_auc: 0.97133868932724, sc: 0,val: 92.9500162601471, Step: 19000\n",
      "St_auc: 0.9716385006904602, sc: 0,val: 92.96000599861145, Step: 20000\n",
      "St_auc: 0.9718509912490845, sc: 0,val: 93.1600034236908, Step: 21000\n",
      "St_auc: 0.972102701663971, sc: 0,val: 93.1800127029419, Step: 22000\n",
      "St_auc: 0.9723095297813416, sc: 0,val: 93.27001571655273, Step: 23000\n",
      "St_auc: 0.9725273847579956, sc: 0,val: 93.35001111030579, Step: 24000\n",
      "St_auc: 0.972724199295044, sc: 0,val: 93.31001043319702, Step: 25000\n",
      "St_auc: 0.9728739261627197, sc: 0,val: 93.28001141548157, Step: 26000\n",
      "St_auc: 0.9730446338653564, sc: 0,val: 93.22001338005066, Step: 27000\n",
      "St_auc: 0.9731445908546448, sc: 0,val: 93.18000674247742, Step: 28000\n",
      "St_auc: 0.9732903838157654, sc: 0,val: 92.99001097679138, Step: 29000\n",
      "St_auc: 0.9734441637992859, sc: 0,val: 93.45001578330994, Step: 30000\n",
      "St_auc: 0.9735826849937439, sc: 0,val: 93.36000680923462, Step: 31000\n",
      "St_auc: 0.9737063646316528, sc: 0,val: 93.39000582695007, Step: 32000\n",
      "St_auc: 0.9738147854804993, sc: 0,val: 93.33000779151917, Step: 33000\n",
      "St_auc: 0.9739233255386353, sc: 0,val: 93.37000846862793, Step: 34000\n",
      "St_auc: 0.974037766456604, sc: 0,val: 93.44000816345215, Step: 35000\n",
      "St_auc: 0.9741114377975464, sc: 0,val: 93.38001012802124, Step: 36000\n",
      "St_auc: 0.9742116332054138, sc: 0,val: 93.32001209259033, Step: 37000\n",
      "St_auc: 0.9743014574050903, sc: 0,val: 93.43001246452332, Step: 38000\n",
      "St_auc: 0.9743932485580444, sc: 0,val: 93.32000613212585, Step: 39000\n",
      "St_auc: 0.9744876027107239, sc: 0,val: 93.29001307487488, Step: 40000\n",
      "St_auc: 0.9745703935623169, sc: 0,val: 93.29001307487488, Step: 41000\n",
      "St_auc: 0.9746388792991638, sc: 0,val: 93.20001602172852, Step: 42000\n",
      "St_auc: 0.9747218489646912, sc: 0,val: 93.26001405715942, Step: 43000\n",
      "St_auc: 0.9748112559318542, sc: 0,val: 93.45000982284546, Step: 44000\n",
      "St_auc: 0.9748936891555786, sc: 0,val: 93.36000680923462, Step: 45000\n",
      "St_auc: 0.9749611616134644, sc: 0,val: 93.1900143623352, Step: 46000\n",
      "St_auc: 0.9750356078147888, sc: 0,val: 93.40001344680786, Step: 47000\n",
      "St_auc: 0.9751020669937134, sc: 0,val: 93.1800127029419, Step: 48000\n",
      "St_auc: 0.9751757979393005, sc: 0,val: 93.38001012802124, Step: 49000\n",
      "St_auc: 0.9752344489097595, sc: 0,val: 93.33000779151917, Step: 50000\n",
      "St_auc: 0.9753003716468811, sc: 0,val: 93.42000484466553, Step: 51000\n",
      "St_auc: 0.9753606915473938, sc: 0,val: 93.34000945091248, Step: 52000\n",
      "St_auc: 0.9754104614257812, sc: 0,val: 93.23000907897949, Step: 53000\n",
      "St_auc: 0.9754598736763, sc: 0,val: 93.21001768112183, Step: 54000\n",
      "St_auc: 0.9755163192749023, sc: 0,val: 93.44000816345215, Step: 55000\n",
      "St_auc: 0.9755669832229614, sc: 0,val: 93.35001111030579, Step: 56000\n",
      "St_auc: 0.9756194949150085, sc: 0,val: 93.42001080513, Step: 57000\n",
      "St_auc: 0.9756760597229004, sc: 0,val: 93.37000846862793, Step: 58000\n",
      "St_auc: 0.9757176041603088, sc: 0,val: 93.41001510620117, Step: 59000\n",
      "St_auc: 0.9757723212242126, sc: 0,val: 93.52000951766968, Step: 60000\n",
      "St_auc: 0.9758174419403076, sc: 0,val: 93.31001043319702, Step: 61000\n",
      "St_auc: 0.9758636951446533, sc: 0,val: 93.20001006126404, Step: 62000\n",
      "St_auc: 0.9759168028831482, sc: 0,val: 93.4900164604187, Step: 63000\n",
      "St_auc: 0.975957453250885, sc: 0,val: 93.25001239776611, Step: 64000\n",
      "St_auc: 0.975957453250885, sc: 1,val: 93.22000741958618, Step: 65000\n",
      "St_auc: 0.9759837985038757, sc: 0,val: 93.50001215934753, Step: 66000\n",
      "St_auc: 0.9760332107543945, sc: 0,val: 93.34000945091248, Step: 67000\n",
      "St_auc: 0.9760692715644836, sc: 0,val: 93.27000975608826, Step: 68000\n",
      "St_auc: 0.9761143326759338, sc: 0,val: 93.43001246452332, Step: 69000\n",
      "St_auc: 0.9761557579040527, sc: 0,val: 93.44000816345215, Step: 70000\n",
      "St_auc: 0.9761859774589539, sc: 0,val: 93.31001043319702, Step: 71000\n",
      "St_auc: 0.9762241244316101, sc: 0,val: 93.48001480102539, Step: 72000\n",
      "St_auc: 0.9762672781944275, sc: 0,val: 93.32001209259033, Step: 73000\n",
      "St_auc: 0.9763060808181763, sc: 0,val: 93.4900164604187, Step: 74000\n",
      "St_auc: 0.9763392210006714, sc: 0,val: 93.31001043319702, Step: 75000\n",
      "St_auc: 0.9763680696487427, sc: 0,val: 93.39001178741455, Step: 76000\n",
      "St_auc: 0.9763967990875244, sc: 0,val: 93.28001141548157, Step: 77000\n",
      "St_auc: 0.976427435874939, sc: 0,val: 93.35000514984131, Step: 78000\n",
      "St_auc: 0.9764569997787476, sc: 0,val: 93.31000447273254, Step: 79000\n",
      "St_auc: 0.9764869809150696, sc: 0,val: 93.41001510620117, Step: 80000\n",
      "St_auc: 0.976512610912323, sc: 0,val: 93.05000305175781, Step: 81000\n",
      "St_auc: 0.9765409231185913, sc: 0,val: 93.36000680923462, Step: 82000\n",
      "St_auc: 0.9765722751617432, sc: 0,val: 93.40001344680786, Step: 83000\n",
      "St_auc: 0.9766035079956055, sc: 0,val: 93.48000288009644, Step: 84000\n",
      "St_auc: 0.9766350984573364, sc: 0,val: 93.42001676559448, Step: 85000\n",
      "St_auc: 0.9766674041748047, sc: 0,val: 93.56001019477844, Step: 86000\n",
      "St_auc: 0.9766840934753418, sc: 0,val: 93.42001676559448, Step: 87000\n",
      "St_auc: 0.9767181873321533, sc: 0,val: 93.56001615524292, Step: 88000\n",
      "St_auc: 0.9767501354217529, sc: 0,val: 93.52001547813416, Step: 89000\n",
      "St_auc: 0.976777195930481, sc: 0,val: 93.39001178741455, Step: 90000\n",
      "St_auc: 0.9768058061599731, sc: 0,val: 93.55001449584961, Step: 91000\n",
      "St_auc: 0.976830780506134, sc: 0,val: 93.43001246452332, Step: 92000\n",
      "St_auc: 0.9768585562705994, sc: 0,val: 93.41002106666565, Step: 93000\n",
      "St_auc: 0.9768751859664917, sc: 0,val: 93.22001338005066, Step: 94000\n",
      "St_auc: 0.9769001007080078, sc: 0,val: 93.35001707077026, Step: 95000\n",
      "St_auc: 0.9769223928451538, sc: 0,val: 93.42001080513, Step: 96000\n",
      "St_auc: 0.9769431948661804, sc: 0,val: 93.47001314163208, Step: 97000\n",
      "St_auc: 0.9769582152366638, sc: 0,val: 93.44000816345215, Step: 98000\n",
      "St_auc: 0.9769812822341919, sc: 0,val: 93.6000108718872, Step: 99000\n",
      "St_auc: 0.9770037531852722, sc: 0,val: 93.45000982284546, Step: 100000\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "for batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2 in zip(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2):\n",
    "    hparams = '_bs={}_ps={}_d1={}_d2={}_d3={}_nh1={}_nh2={}'.format(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2)\n",
    "    run_path, folder_path, count = createFolderstructure()\n",
    "        \n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.2)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=18, inter_op_parallelism_threads=18)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    weights_1 = []\n",
    "    biases_1 = []\n",
    "    \n",
    "    iteration = 1\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))\n",
    "        \n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_c1], stddev=0.1), name='W_1')\n",
    "            layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth_c1]), name='B_1')\n",
    "\n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([23*23*depth_c1, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(iteration)).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "            \n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_1.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_1, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver({\"weight_1\":layer1_weights, \"bias_1\":layer1_biases})\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'First_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_1.append(layer1_weights.eval())\n",
    "        biases_1.append(layer1_biases.eval())\n",
    "\n",
    "        \n",
    "    \n",
    "    weights_2 = []\n",
    "    biases_2 = []\n",
    "    \n",
    "    iteration = 2\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))                    \n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_1[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_1[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "        \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_c1, depth_c2], stddev=0.1), name='W')\n",
    "            layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_c2]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([12*12*depth_c2, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(iteration)).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_2.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "        \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Second_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_2.append(layer1_weights.eval())\n",
    "        weights_2.append(layer2_weights.eval())\n",
    "        biases_2.append(layer1_biases.eval())\n",
    "        biases_2.append(layer2_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    weights_3 = []\n",
    "    biases_3 = []\n",
    "    \n",
    "    iteration = 3\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_2[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_2[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_2[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_2[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_c2, depth_c3], stddev=0.1), name='W')\n",
    "            layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_c3]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([6*6*depth_c3, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Third_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_3.append(layer1_weights.eval())\n",
    "        weights_3.append(layer2_weights.eval())\n",
    "        weights_3.append(layer3_weights.eval())\n",
    "        biases_3.append(layer1_biases.eval())\n",
    "        biases_3.append(layer2_biases.eval())\n",
    "        biases_3.append(layer3_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    weights_4 = []\n",
    "    biases_4 = []\n",
    "    \n",
    "    iteration = 4\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session created')\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_3[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_3[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_3[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_3[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            init_w_3 = tf.constant(weights_3[2])\n",
    "            layer3_weights = tf.get_variable('W_3', initializer=init_w_3, trainable=trainable)\n",
    "            init_b_3 = tf.constant(biases_3[2])\n",
    "            layer3_biases = tf.get_variable('B_3', initializer=init_b_3, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Fourth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_1'.format(iteration)):\n",
    "            layer4_weights = tf.Variable(tf.truncated_normal([6*6*depth_c3, num_hidden_f1], stddev=0.1), name='W')\n",
    "            layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_f1]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([num_hidden_f1, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_1 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(hidden_1, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Fourth_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_4.append(layer1_weights.eval())\n",
    "        weights_4.append(layer2_weights.eval())\n",
    "        weights_4.append(layer3_weights.eval())\n",
    "        weights_4.append(layer4_weights.eval())\n",
    "        biases_4.append(layer1_biases.eval())\n",
    "        biases_4.append(layer2_biases.eval())\n",
    "        biases_4.append(layer3_biases.eval())\n",
    "        biases_4.append(layer4_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    iteration = 5\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session created')\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_4[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_4[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_4[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_4[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            init_w_3 = tf.constant(weights_4[2])\n",
    "            layer3_weights = tf.get_variable('W_3', initializer=init_w_3, trainable=trainable)\n",
    "            init_b_3 = tf.constant(biases_4[2])\n",
    "            layer3_biases = tf.get_variable('B_3', initializer=init_b_3, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Fourth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_1'.format(iteration)):\n",
    "            init_w_4 = tf.constant(weights_4[3])\n",
    "            layer4_weights = tf.get_variable('W_4', initializer=init_w_4, trainable=trainable)\n",
    "            init_b_4 = tf.constant(biases_4[3])\n",
    "            layer4_biases = tf.get_variable('B_4', initializer=init_b_4, trainable=trainable)\n",
    "            \n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Fifth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_2'.format(iteration)):\n",
    "            layer5_weights = tf.Variable(tf.truncated_normal([num_hidden_f1, num_hidden_f2], stddev=0.1), name='W')\n",
    "            layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_f2]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer5_weights)\n",
    "            tf.summary.histogram(\"biases\", layer5_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([num_hidden_f2, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_1 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer5_weights) + layer5_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(hidden_2, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Fifth_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "        acc, stopping_auc, step = training(num_steps_final)\n",
    "\n",
    "        dauer = time.time() - start\n",
    "        early_stopped = True if step < num_steps_final-1 else False\n",
    "        \n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([learning_rate, batch_size, patch_size, [depth_c1, depth_c2, depth_c3], [num_hidden_f1, num_hidden_f2], acc*100, stopping_auc, step, early_stopped, dauer, str(max(count)+1)+'_'+title_name])\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTS: 0.916356110573 0.00613240043811\n",
      "TS: 0.936144304276 0.00673125752898\n",
      "TSM: 0.94522793293 0.00659938181433\n",
      "NTL: nan nan\n",
      "TL: 0.921832829714 0.0133330523968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "not_trainable_short = [0.9255420565605164, 0.9113821983337402, 0.9158235788345337, 0.9085223078727722, 0.9205104112625122]\n",
    "trainable_short = [0.9306667447090149, 0.9477723836898804, 0.9396480321884155, 0.9324250817298889, 0.9302092790603638]\n",
    "trainable_short_many = [0.9484280347824097, 0.9473375678062439, 0.9320979714393616, 0.9493550062179565, 0.9489210844039917]\n",
    "\n",
    "not_trainable_long = []\n",
    "trainable_long = [0.9084997773170471, 0.9351658821105957]\n",
    "\n",
    "print('NTS:', np.mean(not_trainable_short), np.std(not_trainable_short))\n",
    "print('TS:', np.mean(trainable_short), np.std(trainable_short))\n",
    "print('TSM:', np.mean(trainable_short_many), np.std(trainable_short_many))\n",
    "\n",
    "print('NTL:', np.mean(not_trainable_long), np.std(not_trainable_long))\n",
    "print('TL:', np.mean(trainable_long), np.std(trainable_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
