{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data_path = '/fhgfs/users/jbehnken/01_Data/01b_MC_Data_flat' # Path to preprocessed data\n",
    "num_files = 500 # Number of files to load - 1 file = 1000 events\n",
    "events_in_validation = 10000\n",
    "number_of_nets = 1\n",
    "\n",
    "save_model_path = '/fhgfs/users/jbehnken/01_Data/04_Models'\n",
    "model_name = 'pre-cccfff'\n",
    "title_name = 'with_5_100_pre_flat'\n",
    "\n",
    "# Hyperparameter for the model (fit manually)\n",
    "num_labels = 2 # gamma or proton\n",
    "num_channels = 1 # it is a greyscale image\n",
    "\n",
    "num_steps_pretraining = 5000\n",
    "num_steps_final = 100000\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = np.random.randint(64, 257, size=number_of_nets)\n",
    "patch_size = np.random.randint(0, 1, size=number_of_nets)*2 +3\n",
    "\n",
    "depth_c1 = np.random.randint(4, 12, size=number_of_nets)\n",
    "depth_c2 = np.random.randint(4, 12, size=number_of_nets)+depth_c1\n",
    "depth_c3 = np.random.randint(4, 12, size=number_of_nets)+depth_c2\n",
    "\n",
    "num_hidden_f1 = np.random.randint(10, 60, size=number_of_nets)\n",
    "num_hidden_f2 = num_hidden_f1\n",
    "\n",
    "dropout_rate_c = 0.9\n",
    "dropout_rate_c_output = 0.75\n",
    "dropout_rate_f = 0.5\n",
    "\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-35:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-41:\n",
      "Process ForkPoolWorker-33:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-43:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-2:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(mc_data_path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Randomizing the files to load\n",
    "loading_files = os.listdir(mc_data_path)\n",
    "np.random.shuffle(loading_files)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, loading_files[:num_files])\n",
    "pics, labs = zip(*data)\n",
    "\n",
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "\n",
    "test_dataset = pic[p][:events_in_validation]\n",
    "test_labels = lab[p][:events_in_validation]\n",
    "train_dataset = pic[p][events_in_validation:]\n",
    "train_labels = lab[p][events_in_validation:]\n",
    "del p, pic, lab\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFolderstructure():\n",
    "    # Creating folder structure\n",
    "    file_paths = os.listdir(save_model_path)\n",
    "    for path in file_paths:\n",
    "        name = '_' + model_name\n",
    "        if path.endswith(name):\n",
    "            correct_path = path\n",
    "\n",
    "    if 'correct_path' in locals():\n",
    "        folder_path = os.path.join(save_model_path, correct_path)\n",
    "    else:\n",
    "        folder_number = len(os.listdir(save_model_path))+1\n",
    "        folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Learning_Rate','Batch_Size','Patch_Size','Depth','Hidden_Nodes','Accuracy','Auc','Steps', 'Early_Stopped','Time', 'Title'])\n",
    "\n",
    "    models_path = os.path.join(folder_path, 'models_folder')\n",
    "    if not os.path.exists(models_path):\n",
    "        os.mkdir(models_path)\n",
    "\n",
    "    run_folders = os.listdir(models_path)\n",
    "    if len(run_folders)==0:\n",
    "        count = [0]\n",
    "    else:\n",
    "        count = [int(folder.split('_')[0]) for folder in run_folders]\n",
    "\n",
    "    run_path = os.path.join(models_path, str(max(count)+1)+'_'+title_name)\n",
    "    os.mkdir(run_path)\n",
    "    return run_path, folder_path, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(steps):\n",
    "    print('Layer {} training:'.format(iteration))\n",
    "    for step in range(steps+1):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Creating a feed_dict to train the model on in this step\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\n",
    "        opt = sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        # Updating the output to stay in touch with the training process\n",
    "        if (step % 1000 == 0):\n",
    "            [acc, auc, s] = sess.run([test_accuracy, test_auc, summ], feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "            auc_now = auc[1]                        \n",
    "            if step == 0:\n",
    "                stopping_auc = 0.0\n",
    "                sink_count = 0\n",
    "            else:\n",
    "                if auc_now > stopping_auc:\n",
    "                    stopping_auc = auc_now\n",
    "                    sink_count = 0\n",
    "                    saver.save(sess, os.path.join(run_path, 'First_Layer'))\n",
    "                else:\n",
    "                    sink_count += 1\n",
    "            print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, acc*100, step))\n",
    "            if sink_count == 5:\n",
    "                break\n",
    "    return acc, stopping_auc, step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 created\n",
      "Layers created\n",
      "Layer 1 training:\n",
      "St_auc: 0.0, sc: 0,val: 21.879997849464417, Step: 0\n",
      "St_auc: 0.3803589940071106, sc: 0,val: 79.47999835014343, Step: 1000\n",
      "St_auc: 0.5083673596382141, sc: 0,val: 81.40000104904175, Step: 2000\n",
      "St_auc: 0.5882619619369507, sc: 0,val: 81.77000284194946, Step: 3000\n",
      "St_auc: 0.6408799886703491, sc: 0,val: 83.77000689506531, Step: 4000\n",
      "St_auc: 0.68000328540802, sc: 0,val: 83.69000554084778, Step: 5000\n",
      "Session 2 created\n",
      "Layers created\n",
      "Layer 2 training:\n",
      "St_auc: 0.0, sc: 0,val: 78.09000015258789, Step: 0\n",
      "St_auc: 0.8103288412094116, sc: 0,val: 82.19000101089478, Step: 1000\n",
      "St_auc: 0.841317355632782, sc: 0,val: 86.31000518798828, Step: 2000\n",
      "St_auc: 0.8626125454902649, sc: 0,val: 87.4500036239624, Step: 3000\n",
      "St_auc: 0.877315104007721, sc: 0,val: 88.22000026702881, Step: 4000\n",
      "St_auc: 0.8885502815246582, sc: 0,val: 88.09000253677368, Step: 5000\n",
      "Session 3 created\n",
      "Layers created\n",
      "Layer 3 training:\n",
      "St_auc: 0.0, sc: 0,val: 77.38000154495239, Step: 0\n",
      "St_auc: 0.8729799389839172, sc: 0,val: 86.83000802993774, Step: 1000\n",
      "St_auc: 0.9077644348144531, sc: 0,val: 90.15001058578491, Step: 2000\n",
      "St_auc: 0.9232602119445801, sc: 0,val: 89.83001112937927, Step: 3000\n",
      "St_auc: 0.931989848613739, sc: 0,val: 90.32000303268433, Step: 4000\n",
      "St_auc: 0.9377949833869934, sc: 0,val: 91.6400134563446, Step: 5000\n",
      "Session created\n",
      "Layers created\n",
      "Layer 4 training:\n",
      "St_auc: 0.0, sc: 0,val: 76.69000029563904, Step: 0\n",
      "St_auc: 0.910575270652771, sc: 0,val: 91.44001007080078, Step: 1000\n",
      "St_auc: 0.9341013431549072, sc: 0,val: 92.0400083065033, Step: 2000\n",
      "St_auc: 0.9437044858932495, sc: 0,val: 91.73001050949097, Step: 3000\n",
      "St_auc: 0.9493384957313538, sc: 0,val: 91.87000393867493, Step: 4000\n",
      "St_auc: 0.9534915089607239, sc: 0,val: 92.39001274108887, Step: 5000\n",
      "Session created\n",
      "Layers created\n",
      "Layer 5 training:\n",
      "St_auc: 0.0, sc: 0,val: 14.409998059272766, Step: 0\n",
      "St_auc: 0.719871997833252, sc: 0,val: 92.19000935554504, Step: 1000\n",
      "St_auc: 0.8474664688110352, sc: 0,val: 92.61000156402588, Step: 2000\n",
      "St_auc: 0.8944573998451233, sc: 0,val: 92.45001077651978, Step: 3000\n",
      "St_auc: 0.9168676137924194, sc: 0,val: 92.00000762939453, Step: 4000\n",
      "St_auc: 0.9303593635559082, sc: 0,val: 92.64000654220581, Step: 5000\n",
      "St_auc: 0.9390187859535217, sc: 0,val: 92.89000630378723, Step: 6000\n",
      "St_auc: 0.9449895620346069, sc: 0,val: 92.77001023292542, Step: 7000\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function WritableFile_Close> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-787cac4ffa46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fifth_Layer'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mdauer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0c3d1d022c88>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(steps)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mstopping_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc_now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0msink_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'First_Layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0msink_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1373\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents)\u001b[0m\n\u001b[1;32m    350\u001b[0m   \"\"\"\n\u001b[1;32m    351\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m   \u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[0;34m(filename, file_content)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \"\"\"\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mret_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSet_TF_Status_from_Status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36mClose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWritableFile_Close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mFlush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function WritableFile_Close> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "for batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2 in zip(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2):\n",
    "    hparams = '_bs={}_ps={}_d1={}_d2={}_d3={}_nh1={}_nh2={}'.format(batch_size, patch_size, depth_c1, depth_c2, depth_c3, num_hidden_f1, num_hidden_f2)\n",
    "    run_path, folder_path, count = createFolderstructure()\n",
    "        \n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.2)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=18, inter_op_parallelism_threads=18)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    weights_1 = []\n",
    "    biases_1 = []\n",
    "    \n",
    "    iteration = 1\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))\n",
    "        \n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_c1], stddev=0.1), name='W_1')\n",
    "            layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth_c1]), name='B_1')\n",
    "\n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([23*23*depth_c1, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(iteration)).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "            \n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_1.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_1, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver({\"weight_1\":layer1_weights, \"bias_1\":layer1_biases})\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'First_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_1.append(layer1_weights.eval())\n",
    "        biases_1.append(layer1_biases.eval())\n",
    "\n",
    "        \n",
    "    \n",
    "    weights_2 = []\n",
    "    biases_2 = []\n",
    "    \n",
    "    iteration = 2\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))                    \n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_1[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_1[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "        \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_c1, depth_c2], stddev=0.1), name='W')\n",
    "            layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_c2]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([12*12*depth_c2, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, name='{}_adam'.format(iteration)).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_2.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "        \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Second_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_2.append(layer1_weights.eval())\n",
    "        weights_2.append(layer2_weights.eval())\n",
    "        biases_2.append(layer1_biases.eval())\n",
    "        biases_2.append(layer2_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    weights_3 = []\n",
    "    biases_3 = []\n",
    "    \n",
    "    iteration = 3\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session {} created'.format(iteration))\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_2[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_2[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_2[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_2[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_c2, depth_c3], stddev=0.1), name='W')\n",
    "            layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_c3]), name='B')\n",
    "\n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([6*6*depth_c3, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(reshape, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(reshape, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Third_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_3.append(layer1_weights.eval())\n",
    "        weights_3.append(layer2_weights.eval())\n",
    "        weights_3.append(layer3_weights.eval())\n",
    "        biases_3.append(layer1_biases.eval())\n",
    "        biases_3.append(layer2_biases.eval())\n",
    "        biases_3.append(layer3_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    weights_4 = []\n",
    "    biases_4 = []\n",
    "    \n",
    "    iteration = 4\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session created')\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_3[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_3[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_3[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_3[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            init_w_3 = tf.constant(weights_3[2])\n",
    "            layer3_weights = tf.get_variable('W_3', initializer=init_w_3, trainable=trainable)\n",
    "            init_b_3 = tf.constant(biases_3[2])\n",
    "            layer3_biases = tf.get_variable('B_3', initializer=init_b_3, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Fourth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_1'.format(iteration)):\n",
    "            layer4_weights = tf.Variable(tf.truncated_normal([6*6*depth_c3, num_hidden_f1], stddev=0.1), name='W')\n",
    "            layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_f1]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([num_hidden_f1, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_1 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(hidden_1, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Fourth_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        training(num_steps_pretraining)\n",
    "        \n",
    "        weights_4.append(layer1_weights.eval())\n",
    "        weights_4.append(layer2_weights.eval())\n",
    "        weights_4.append(layer3_weights.eval())\n",
    "        weights_4.append(layer4_weights.eval())\n",
    "        biases_4.append(layer1_biases.eval())\n",
    "        biases_4.append(layer2_biases.eval())\n",
    "        biases_4.append(layer3_biases.eval())\n",
    "        biases_4.append(layer4_biases.eval())\n",
    "        \n",
    "        \n",
    "        \n",
    "    iteration = 5\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        print('Session created')\n",
    "            \n",
    "        # Create tf.variables for the three different datasets\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "        tf_test_labels = tf.constant(test_labels, name='test_labels')\n",
    "        \n",
    "        # First layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_1'.format(iteration)):\n",
    "            init_w_1 = tf.constant(weights_4[0])\n",
    "            layer1_weights = tf.get_variable('W_1', initializer=init_w_1, trainable=trainable)\n",
    "            init_b_1 = tf.constant(biases_4[0])\n",
    "            layer1_biases = tf.get_variable('B_1', initializer=init_b_1, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer1_weights)\n",
    "            tf.summary.histogram(\"biases\", layer1_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Second layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_2'.format(iteration)):\n",
    "            init_w_2 = tf.constant(weights_4[1])\n",
    "            layer2_weights = tf.get_variable('W_2', initializer=init_w_2, trainable=trainable)\n",
    "            init_b_2 = tf.constant(biases_4[1])\n",
    "            layer2_biases = tf.get_variable('B_2', initializer=init_b_2, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer2_weights)\n",
    "            tf.summary.histogram(\"biases\", layer2_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Third layer is a convolution layer\n",
    "        with tf.name_scope('{}_conv2d_3'.format(iteration)):\n",
    "            init_w_3 = tf.constant(weights_4[2])\n",
    "            layer3_weights = tf.get_variable('W_3', initializer=init_w_3, trainable=trainable)\n",
    "            init_b_3 = tf.constant(biases_4[2])\n",
    "            layer3_biases = tf.get_variable('B_3', initializer=init_b_3, trainable=trainable)\n",
    "            \n",
    "            conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME') \n",
    "            hidden = tf.nn.relu(conv + layer3_biases)\n",
    "            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool = tf.nn.dropout(pool, dropout_rate_c_output)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer3_weights)\n",
    "            tf.summary.histogram(\"biases\", layer3_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # The reshape produces an input vector for the dense layer\n",
    "        with tf.name_scope('{}_reshape'.format(iteration)):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # Fourth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_1'.format(iteration)):\n",
    "            init_w_4 = tf.constant(weights_4[3])\n",
    "            layer4_weights = tf.get_variable('W_4', initializer=init_w_4, trainable=trainable)\n",
    "            init_b_4 = tf.constant(biases_4[3])\n",
    "            layer4_biases = tf.get_variable('B_4', initializer=init_b_4, trainable=trainable)\n",
    "            \n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer4_weights)\n",
    "            tf.summary.histogram(\"biases\", layer4_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Fifth layer is a dense layer\n",
    "        with tf.name_scope('{}_fc_2'.format(iteration)):\n",
    "            layer5_weights = tf.Variable(tf.truncated_normal([num_hidden_f1, num_hidden_f2], stddev=0.1), name='W')\n",
    "            layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_f2]), name='B')\n",
    "\n",
    "            hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "            hidden = tf.nn.dropout(hidden, dropout_rate_f)\n",
    "\n",
    "            tf.summary.histogram(\"weights\", layer5_weights)\n",
    "            tf.summary.histogram(\"biases\", layer5_biases)\n",
    "            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "        # Output layer is a dense layer\n",
    "        with tf.name_scope('{}_Output'.format(iteration)):\n",
    "            output_weights = tf.Variable(tf.truncated_normal([num_hidden_f2, num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(hidden, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('{}_loss'.format(iteration)):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('{}_optimizer'.format(iteration)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('{}_prediction'.format(iteration)):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "        # Evaluating the network: accuracy\n",
    "        with tf.name_scope('{}_test'.format(iteration)):\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_3 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_2, layer3_weights, [1, 1, 1, 1], padding='SAME') + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool_3.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool_3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_1 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "            hidden_2 = tf.nn.relu(tf.matmul(hidden_1, layer5_weights) + layer5_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(hidden_2, output_weights) + output_biases)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)                        \n",
    "\n",
    "        # Evaluating the network: auc\n",
    "        with tf.name_scope('{}_auc'.format(iteration)):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            tf.summary.scalar('test_auc_1', test_auc[1])\n",
    "        print('Layers created')\n",
    "            \n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(os.path.join(run_path, 'Fifth_Layer'+hparams))\n",
    "        writer.add_graph(sess.graph)\n",
    "        acc, stopping_auc, step = training(num_steps_final)\n",
    "\n",
    "        dauer = time.time() - start\n",
    "        early_stopped = True if step < num_steps_final-1 else False\n",
    "        \n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([learning_rate, batch_size, patch_size, [depth_c1, depth_c2, depth_c3], [num_hidden_f1, num_hidden_f2], acc*100, stopping_auc, step, early_stopped, dauer, str(max(count)+1)+'_'+title_name])\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTS: 0.916356110573 0.00613240043811\n",
      "TS: 0.936144304276 0.00673125752898\n",
      "TSM: 0.94522793293 0.00659938181433\n",
      "NTL: nan nan\n",
      "TL: 0.921832829714 0.0133330523968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "not_trainable_short = [0.9255420565605164, 0.9113821983337402, 0.9158235788345337, 0.9085223078727722, 0.9205104112625122]\n",
    "trainable_short = [0.9306667447090149, 0.9477723836898804, 0.9396480321884155, 0.9324250817298889, 0.9302092790603638]\n",
    "trainable_short_many = [0.9484280347824097, 0.9473375678062439, 0.9320979714393616, 0.9493550062179565, 0.9489210844039917]\n",
    "\n",
    "not_trainable_long = []\n",
    "trainable_long = [0.9084997773170471, 0.9351658821105957]\n",
    "\n",
    "print('NTS:', np.mean(not_trainable_short), np.std(not_trainable_short))\n",
    "print('TS:', np.mean(trainable_short), np.std(trainable_short))\n",
    "print('TSM:', np.mean(trainable_short_many), np.std(trainable_short_many))\n",
    "\n",
    "print('NTL:', np.mean(not_trainable_long), np.std(not_trainable_long))\n",
    "print('TL:', np.mean(trainable_long), np.std(trainable_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
