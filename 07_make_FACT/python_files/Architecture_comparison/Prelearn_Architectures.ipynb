{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import h5py\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createFolders(model_name, save_model_path):\n",
    "    # Iterates over all existing models and chooses the right folder to save everything \n",
    "    file_paths = os.listdir(save_model_path)\n",
    "    for path in file_paths:\n",
    "        name = '_' + model_name\n",
    "        if path.endswith(name):\n",
    "            correct_path = path \n",
    "\n",
    "    # Creates missing folders or chooses the right one to append new data to\n",
    "    if 'correct_path' in locals():\n",
    "        folder_path = os.path.join(save_model_path, correct_path)\n",
    "    else:\n",
    "        folder_number = len(os.listdir(save_model_path))+1\n",
    "        folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "        # Creates the csv to save every models performance in\n",
    "        c_count = model_name.count('c')\n",
    "        depth_names = []\n",
    "        for i in range(c_count):\n",
    "            depth_names.append('Depth_{}'.format(i+1))\n",
    "        columns = ['Learning_Rate','Batch_Size','Patch_Size']\n",
    "        columns.extend(depth_names)\n",
    "        columns.extend(['Hidden_Nodes','Accuracy','Auc','Steps', 'Early_Stopped','Time', 'Title'])\n",
    "\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def metaYielder(path_mc_images):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        keys = list(f.keys())\n",
    "        events = []\n",
    "        for key in keys:\n",
    "            events.append(len(f[key]))\n",
    "            \n",
    "    gamma_anteil = events[0]/np.sum(events)\n",
    "    hadron_anteil = events[1]/np.sum(events)\n",
    "    \n",
    "    gamma_count = int(round(num_events*gamma_anteil))\n",
    "    hadron_count = int(round(num_events*hadron_anteil))\n",
    "    \n",
    "    return gamma_anteil, hadron_anteil, gamma_count, hadron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batchYielder():\n",
    "    gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder()\n",
    "\n",
    "    gamma_batch_size = int(round(batch_size*gamma_anteil))\n",
    "    hadron_batch_size = int(round(batch_size*hadron_anteil))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        gamma_offset = (step * gamma_batch_size) % (gamma_count - gamma_batch_size)\n",
    "        hadron_offset = (step * hadron_batch_size) % (hadron_count - hadron_batch_size)\n",
    "\n",
    "        with h5py.File(path_mc_images, 'r') as f:\n",
    "            gamma_data = f['Gamma'][gamma_offset:(gamma_offset + gamma_batch_size), :, :, :]\n",
    "            hadron_data = f['Hadron'][hadron_offset:(hadron_offset + hadron_batch_size), :, :, :]\n",
    "\n",
    "        batch_data = np.concatenate((gamma_data, hadron_data), axis=0)\n",
    "        labels = np.array([True]*gamma_batch_size+[False]*hadron_batch_size)\n",
    "        batch_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        gamma_size = int(round(events_in_validation_and_testing*gamma_anteil))\n",
    "        hadron_size = int(round(events_in_validation_and_testing*hadron_anteil))\n",
    "\n",
    "        gamma_valid_data = f['Gamma'][gamma_count:(gamma_count+gamma_size), :, :, :]\n",
    "        hadron_valid_data = f['Hadron'][hadron_count:(hadron_count+hadron_size), :, :, :]\n",
    "\n",
    "        valid_dataset = np.concatenate((gamma_valid_data, hadron_valid_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        valid_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "\n",
    "        gamma_test_data = f['Gamma'][(gamma_count+gamma_size):(gamma_count+2*gamma_size), :, :, :]\n",
    "        hadron_test_data = f['Hadron'][(hadron_count+hadron_size):(hadron_count+2*hadron_size), :, :, :]\n",
    "\n",
    "        test_dataset = np.concatenate((gamma_test_data, hadron_test_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        test_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "        \n",
    "    return valid_dataset, valid_labels, test_dataset, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bestAuc(folder_path, architecture):\n",
    "    # Loading the existing runs to find the best auc untill now. Only a model with a better auc will be saved\n",
    "    df = pd.read_csv(os.path.join(folder_path, architecture+'_Hyperparameter.csv'))\n",
    "    if len(df['Auc']) > 0:\n",
    "        best_auc = df['Auc'].max()\n",
    "    else:\n",
    "        best_auc = 0\n",
    "        \n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getHyperparameter(architecture, number_of_nets):\n",
    "    # Hyperparameter for the model (fit manually)\n",
    "    num_labels = 2 # gamma or proton\n",
    "    num_channels = 1 # it is a greyscale image\n",
    "    \n",
    "    num_steps = 20001     # Maximum batches for the model\n",
    "    \n",
    "    min_batch_size = 64   # How many images will be in a batch\n",
    "    max_batch_size = 257\n",
    "    \n",
    "    patch_size = [3, 5]   # Will the kernel/patch be 3x3 or 5x5\n",
    "\n",
    "    min_depth = 2         # Setting the depth of the convolution layers. New layers will be longer than the preceding\n",
    "    max_depth = 21\n",
    "    \n",
    "    min_num_hidden = 8    # Number of hidden nodes in f-layers. all f-layers will have the same number of nodes\n",
    "    max_num_hidden = 257\n",
    "    \n",
    "    \n",
    "    num_steps = [num_steps] * number_of_nets\n",
    "    batch_size = np.random.randint(min_batch_size, max_batch_size, size=number_of_nets)\n",
    "    patch_size = np.random.choice(patch_size, size=number_of_nets)\n",
    "    layer = architecture[:-1]\n",
    "\n",
    "    depth = []\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets)) # 2 - 21\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[0])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[1])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[2])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[3])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[4])\n",
    "\n",
    "    num_hidden = np.random.randint(min_num_hidden, max_num_hidden, size=number_of_nets)\n",
    "    \n",
    "    # Combining the hyperparameters to fit them into a for-loop\n",
    "    hyperparameter = zip(num_steps, batch_size, patch_size, zip(*depth), num_hidden)\n",
    "    \n",
    "    return num_labels, num_channels, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getSessConf(per_process_gpu_memory_fraction = 0.1, op_parallelism_threads = 18):\n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=per_process_gpu_memory_fraction)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=op_parallelism_threads, inter_op_parallelism_threads=op_parallelism_threads)\n",
    "    \n",
    "    return session_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initWeightsBiases():\n",
    "    # Maximal 6 Convolution Layers & 5 Fully Connectd Layers\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    conv_size_dict={1:23*23, 2:12*12, 3:6*6, 4:3*3, 5:2*2, 6:1*1}\n",
    "    with tf.Session(config=getSessConf()) as sess:\n",
    "        weights_biases = []\n",
    "\n",
    "        if len(depth)>=1:\n",
    "            conv2d_1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth[0]], stddev=0.1), name='W')\n",
    "            conv2d_1_biases = tf.Variable(tf.constant(1.0, shape=[depth[0]]), name='B')\n",
    "\n",
    "        if len(depth)>=2:\n",
    "            conv2d_2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[0], depth[1]], stddev=0.1), name='W')\n",
    "            conv2d_2_biases = tf.Variable(tf.constant(1.0, shape=[depth[1]]), name='B')\n",
    "\n",
    "        if len(depth)>=3:\n",
    "            conv2d_3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[1], depth[2]], stddev=0.1), name='W')\n",
    "            conv2d_3_biases = tf.Variable(tf.constant(1.0, shape=[depth[2]]), name='B')\n",
    "\n",
    "        if len(depth)>=4:\n",
    "            conv2d_4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[2], depth[3]], stddev=0.1), name='W')\n",
    "            conv2d_4_biases = tf.Variable(tf.constant(1.0, shape=[depth[3]]), name='B')\n",
    "\n",
    "        if len(depth)>=5:\n",
    "            conv2d_5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[3], depth[4]], stddev=0.1), name='W')\n",
    "            conv2d_5_biases = tf.Variable(tf.constant(1.0, shape=[depth[4]]), name='B')\n",
    "\n",
    "        if len(depth)>=6:\n",
    "            conv2d_6_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[4], depth[5]], stddev=0.1), name='W')\n",
    "            conv2d_6_biases = tf.Variable(tf.constant(1.0, shape=[depth[5]]), name='B')\n",
    "\n",
    "        shape = conv_size_dict[c_count]*depth[-1]\n",
    "\n",
    "        if f_count-1>=1:\n",
    "            fc_1_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_1_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=2:\n",
    "            fc_2_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=3:\n",
    "            fc_3_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=4:\n",
    "            fc_4_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "            \n",
    "        # Output Layers\n",
    "        if c_count>=1:\n",
    "            shape = conv_size_dict[1]*depth[0]\n",
    "            out_1_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_1_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=2:\n",
    "            shape = conv_size_dict[2]*depth[1]\n",
    "            out_2_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_2_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=3:\n",
    "            shape = conv_size_dict[3]*depth[2]\n",
    "            out_3_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_3_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=4:\n",
    "            shape = conv_size_dict[4]*depth[3]\n",
    "            out_4_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=5:\n",
    "            shape = conv_size_dict[5]*depth[4]\n",
    "            out_5_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=6:\n",
    "            shape = conv_size_dict[6]*depth[5]\n",
    "            out_6_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=1:\n",
    "            shape = num_hidden\n",
    "            out_7_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_7_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=2:\n",
    "            shape = num_hidden\n",
    "            out_8_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_8_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=3:\n",
    "            shape = num_hidden\n",
    "            out_9_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_9_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=4:\n",
    "            shape = num_hidden\n",
    "            out_10_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_10_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if len(depth)>=1:\n",
    "            weights_biases.append([conv2d_1_weights.eval(), conv2d_1_biases.eval()])\n",
    "        if len(depth)>=2:\n",
    "            weights_biases.append([conv2d_2_weights.eval(), conv2d_2_biases.eval()])\n",
    "        if len(depth)>=3:\n",
    "            weights_biases.append([conv2d_3_weights.eval(), conv2d_3_biases.eval()])\n",
    "        if len(depth)>=4:\n",
    "            weights_biases.append([conv2d_4_weights.eval(), conv2d_4_biases.eval()])\n",
    "        if len(depth)>=5:\n",
    "            weights_biases.append([conv2d_5_weights.eval(), conv2d_5_biases.eval()])\n",
    "        if len(depth)>=6:\n",
    "            weights_biases.append([conv2d_6_weights.eval(), conv2d_6_biases.eval()])\n",
    "\n",
    "        if f_count-1>=1:\n",
    "            weights_biases.append([fc_1_weights.eval(), fc_1_biases.eval()])\n",
    "        if f_count-1>=2:\n",
    "            weights_biases.append([fc_2_weights.eval(), fc_2_biases.eval()])\n",
    "        if f_count-1>=3:\n",
    "            weights_biases.append([fc_3_weights.eval(), fc_3_biases.eval()])\n",
    "        if f_count-1>=4:\n",
    "            weights_biases.append([fc_4_weights.eval(), fc_4_biases.eval()])\n",
    "            \n",
    "        if c_count>=1:\n",
    "            weights_biases.append([out_1_weights.eval(), out_1_biases.eval()])\n",
    "        if c_count>=2:\n",
    "            weights_biases.append([out_2_weights.eval(), out_2_biases.eval()])\n",
    "        if c_count>=3:\n",
    "            weights_biases.append([out_3_weights.eval(), out_3_biases.eval()])\n",
    "        if c_count>=4:\n",
    "            weights_biases.append([out_4_weights.eval(), out_4_biases.eval()])\n",
    "        if c_count>=5:\n",
    "            weights_biases.append([out_5_weights.eval(), out_5_biases.eval()])\n",
    "        if c_count>=6:\n",
    "            weights_biases.append([out_6_weights.eval(), out_6_biases.eval()])\n",
    "        if f_count-1>=1:\n",
    "            weights_biases.append([out_7_weights.eval(), out_7_biases.eval()])\n",
    "        if f_count-1>=2:\n",
    "            weights_biases.append([out_8_weights.eval(), out_8_biases.eval()])\n",
    "        if f_count-1>=3:\n",
    "            weights_biases.append([out_9_weights.eval(), out_9_biases.eval()])\n",
    "        if f_count-1>=4:\n",
    "            weights_biases.append([out_01_weights.eval(), out_10_biases.eval()])\n",
    "        \n",
    "\n",
    "        #weights_biases.append([fc_5_weights.eval(), fc_5_biases.eval()])\n",
    "\n",
    "    return weights_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input arguments from outside\n",
    "#path_mc_images = sys.argv[1]\n",
    "#save_model_path = sys.argv[2]\n",
    "\n",
    "path_mc_images = '/fhgfs/users/jbehnken/make_Data/MC_diffuse_flat_preprocessed_images.h5'\n",
    "save_model_path = '/fhgfs/users/jbehnken/crap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " /fhgfs/users/jbehnken/crap/1_cf\n",
      "\n",
      " 20001 147 5 (4,) 212\n",
      "Initialized\n",
      "Pretraining: c\n",
      "Conv-Layer 1 initialized\n",
      "Output-Layer initialized\n",
      "Pretraining: cf\n",
      "Conv-Layer 1 initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 4 for 'output_layer/MatMul' (op: 'MatMul') with input shapes: [147,2116], [5,5,1,4].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 4 for 'output_layer/MatMul' (op: 'MatMul') with input shapes: [147,2116], [5,5,1,4].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7e796e5fce84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m                     \u001b[0mlayerout_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B_out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_b_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayerout_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayerout_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1765\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   \"\"\"\n\u001b[1;32m   1453\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1454\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1455\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 4 for 'output_layer/MatMul' (op: 'MatMul') with input shapes: [147,2116], [5,5,1,4]."
     ]
    }
   ],
   "source": [
    "# Number of events in training-dataset\n",
    "num_events = 5000\n",
    "\n",
    "# Number of events in validation-/test-dataset\n",
    "events_in_validation_and_testing = 1000\n",
    "\n",
    "# Number of nets to compute\n",
    "number_of_nets = 1\n",
    "\n",
    "# Comment on the run\n",
    "title_name = 'test_test'\n",
    "\n",
    "# Architectures to test\n",
    "test_architectures = ['cf', 'cfff', 'ccf', 'ccfff', 'cccfff', 'cccccffff']\n",
    "\n",
    "\n",
    "\n",
    "gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder(path_mc_images)\n",
    "valid_dataset, valid_labels, test_dataset, test_labels = getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count)\n",
    "\n",
    "\n",
    "for architecture in test_architectures:\n",
    "    c_count = architecture.count('c')\n",
    "    f_count = architecture.count('f')\n",
    "    folder_path = createFolders(architecture, save_model_path)\n",
    "    print('\\n\\n', folder_path)\n",
    "    \n",
    "    best_auc = bestAuc(folder_path, architecture)\n",
    "        \n",
    "    num_labels, num_channels, hyperparameter = getHyperparameter(architecture, number_of_nets)\n",
    "\n",
    "    for num_steps, batch_size, patch_size, depth, num_hidden in hyperparameter:\n",
    "        print('\\n', num_steps, batch_size, patch_size, depth, num_hidden)\n",
    "        \n",
    "        weights_biases = initWeightsBiases()\n",
    "        print('Initialized')\n",
    "        \n",
    "        for pretraining_step in range(len(architecture)):\n",
    "            print('Pretraining: {}'.format(architecture[:pretraining_step+1]))\n",
    "            \n",
    "            \n",
    "            # Measuring the loop-time\n",
    "            start = time.time()\n",
    "            # Path to logfiles and correct file name\n",
    "            LOGDIR = '/fhgfs/users/jbehnken/tf_logs/small_logs'\n",
    "            # Getting the right count-number for the new logfiles\n",
    "            logcount = str(len(os.listdir(LOGDIR)))\n",
    "            hparams = '_bs={}_ps={}_d={}_nh={}_ns={}'.format(batch_size, patch_size, depth, num_hidden, num_steps)\n",
    "            \n",
    "            \n",
    "            tf.reset_default_graph()\n",
    "            with tf.Session(config=getSessConf()) as sess:\n",
    "                # Create tf.variables for the three different datasets\n",
    "                tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "                tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "                tf_valid_dataset = tf.constant(valid_dataset, name='valid_data')\n",
    "                tf_valid_labels = tf.constant(valid_labels, name='valid_labels')\n",
    "\n",
    "                tf_test_dataset_final = tf.constant(test_dataset, name='test_data_final')\n",
    "                tf_test_labels_final = tf.constant(test_labels, name='test_labels_final')                    \n",
    "\n",
    "                # Summary for same example input images\n",
    "                tf.summary.image('input', tf_train_dataset, 6)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Creating the graph. Only layers specified in 'model_name' will be added and correctly sized\n",
    "                layer = architecture[:-1]\n",
    "\n",
    "                if layer and layer[0]=='c' and pretraining_step>=0:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_1'):\n",
    "                        init_w_1 = tf.constant(weights_biases[0][0])\n",
    "                        layer1_weights = tf.get_variable('W_1', initializer=init_w_1)\n",
    "                        init_b_1 = tf.constant(weights_biases[0][1])\n",
    "                        layer1_biases = tf.get_variable('B_1', initializer=init_b_1)\n",
    "\n",
    "                        conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer1_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer1_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 1 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='c' and pretraining_step>=1:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_2'):\n",
    "                        init_w_2 = tf.constant(weights_biases[1][0])\n",
    "                        layer2_weights = tf.get_variable('W_2', initializer=init_w_2)\n",
    "                        init_b_2 = tf.constant(weights_biases[1][1])\n",
    "                        layer2_biases = tf.get_variable('B_2', initializer=init_b_2)\n",
    "\n",
    "                        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer2_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer2_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 2 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='c' and pretraining_step>=2:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_3'):\n",
    "                        init_w_3 = tf.constant(weights_biases[2][0])\n",
    "                        layer3_weights = tf.get_variable('W_3', initializer=init_w_3)\n",
    "                        init_b_3 = tf.constant(weights_biases[2][1])\n",
    "                        layer3_biases = tf.get_variable('B_3', initializer=init_b_3)\n",
    "\n",
    "                        conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer3_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer3_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 3 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='c' and pretraining_step>=3:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_4'):\n",
    "                        init_w_4 = tf.constant(weights_biases[3][0])\n",
    "                        layer4_weights = tf.get_variable('W_4', initializer=init_w_4)\n",
    "                        init_b_4 = tf.constant(weights_biases[3][1])\n",
    "                        layer4_biases = tf.get_variable('B_4', initializer=init_b_4)\n",
    "\n",
    "                        conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer4_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer4_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 4 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='c' and pretraining_step>=4:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_5'):\n",
    "                        init_w_5 = tf.constant(weights_biases[4][0])\n",
    "                        layer5_weights = tf.get_variable('W_5', initializer=init_w_5)\n",
    "                        init_b_5 = tf.constant(weights_biases[4][1])\n",
    "                        layer5_biases = tf.get_variable('B_5', initializer=init_b_5)\n",
    "\n",
    "                        conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer5_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer5_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 5 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='c' and pretraining_step>=5:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('conv2d_6'):\n",
    "                        init_w_6 = tf.constant(weights_biases[5][0])\n",
    "                        layer6_weights = tf.get_variable('W_6', initializer=init_w_6)\n",
    "                        init_b_6 = tf.constant(weights_biases[5][1])\n",
    "                        layer6_biases = tf.get_variable('B_6', initializer=init_b_6)\n",
    "\n",
    "                        conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                        hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer6_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer6_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        tf.summary.histogram(\"pooling\", pool)\n",
    "                        \n",
    "                        print('Conv-Layer 6 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                # Reshape convolution layers to process the nodes further with connected layers\n",
    "                with tf.name_scope('reshape'):\n",
    "                    shape = pool.get_shape().as_list()\n",
    "                    output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                if layer and layer[0]=='f' and pretraining_step>=c_count:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('fc_7'):\n",
    "                        init_w_7 = tf.constant(weights_biases[c_count][0])\n",
    "                        layer7_weights = tf.get_variable('W_7', initializer=init_w_7)\n",
    "                        init_b_7 = tf.constant(weights_biases[c_count][1])\n",
    "                        layer7_biases = tf.get_variable('B_7', initializer=init_b_7)\n",
    "\n",
    "                        hidden = tf.nn.relu(tf.matmul(output, layer7_weights) + layer7_biases)\n",
    "                        output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer7_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer7_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        \n",
    "                        print('Fc-Layer 1 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='f' and pretraining_step>=c_count+1:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('fc_8'):\n",
    "                        init_w_8 = tf.constant(weights_biases[c_count+1][0])\n",
    "                        layer8_weights = tf.get_variable('W_8', initializer=init_w_8)\n",
    "                        init_b_8 = tf.constant(weights_biases[c_count+1][1])\n",
    "                        layer8_biases = tf.get_variable('B_8', initializer=init_b_8)\n",
    "\n",
    "                        hidden = tf.nn.relu(tf.matmul(output, layer8_weights) + layer8_biases)\n",
    "                        output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer8_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer8_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        \n",
    "                        print('Fc-Layer 2 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='f' and pretraining_step>=c_count+2:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('fc_9'):\n",
    "                        init_w_9 = tf.constant(weights_biases[c_count+2][0])\n",
    "                        layer9_weights = tf.get_variable('W_9', initializer=init_w_9)\n",
    "                        init_b_9 = tf.constant(weights_biases[c_count+2][1])\n",
    "                        layer9_biases = tf.get_variable('B_9', initializer=init_b_9)\n",
    "\n",
    "                        hidden = tf.nn.relu(tf.matmul(output, layer9_weights) + layer9_biases)\n",
    "                        output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer9_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer9_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        \n",
    "                        print('Fc-Layer 3 initialized')\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                if layer and layer[0]=='f' and pretraining_step>=c_count+3:\n",
    "                    layer = layer[1:]\n",
    "                    with tf.name_scope('fc_10'):\n",
    "                        init_w_10 = tf.constant(weights_biases[c_count+3][0])\n",
    "                        layer10_weights = tf.get_variable('W_10', initializer=init_w_10)\n",
    "                        init_b_10 = tf.constant(weights_biases[c_count+3][1])\n",
    "                        layer10_biases = tf.get_variable('B_10', initializer=init_b_10)\n",
    "\n",
    "                        hidden = tf.nn.relu(tf.matmul(output, layer10_weights) + layer10_biases)\n",
    "                        output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                        tf.summary.histogram(\"weights\", layer10_weights)\n",
    "                        tf.summary.histogram(\"biases\", layer10_biases)\n",
    "                        tf.summary.histogram(\"activations\", hidden)\n",
    "                        \n",
    "                        print('Fc-Layer 4 initialized')\n",
    "                        \n",
    "                        \n",
    "                with tf.name_scope('output_layer'):\n",
    "                    i = len(architecture)-1-pretraining_step\n",
    "                    \n",
    "                    init_w_out = tf.constant(weights_biases[-i][0])\n",
    "                    layerout_weights = tf.get_variable('W_out', initializer=init_w_out)\n",
    "                    init_b_out = tf.constant(weights_biases[i][-1])\n",
    "                    layerout_biases = tf.get_variable('B_out', initializer=init_b_out)\n",
    "\n",
    "                    hidden = tf.nn.relu(tf.matmul(output, layerout_weights) + layerout_biases)\n",
    "                    output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                    tf.summary.histogram(\"weights\", layerout_weights)\n",
    "                    tf.summary.histogram(\"biases\", layerout_biases)\n",
    "                    tf.summary.histogram(\"activations\", hidden)\n",
    "                        \n",
    "                    print('Output-Layer initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8993/529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23*23\n",
    "12*12\n",
    "6*6\n",
    "3*3\n",
    "2*2\n",
    "1*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conv_size_dict={1:23*23, 2:12*12, 3:6*6, 4:3*3, 5:2*2, 6:1*1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 4\n",
      "Tensors: 2\n",
      "Tensor-Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Layer: {}'.format(len(weights_biases)))\n",
    "\n",
    "print('Tensors: {}'.format(len(weights_biases[0])))\n",
    "\n",
    "print('Tensor-Type: {}'.format(type(weights_biases[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Main loop with the training process\n",
    "for num_steps, learning_rate, batch_size, patch_size, depth, num_hidden in hyperparameter:\n",
    "    try:\n",
    "        \n",
    "        # Creating the graph. Only layers specified in 'model_name' will be added and correctly sized\n",
    "        layer = model_name[:-1]\n",
    "        \n",
    "        if layer and layer[0]=='c':\n",
    "            layer = layer[1:]\n",
    "            with tf.name_scope('conv2d_1'):\n",
    "                conv2d_1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth[0]], stddev=0.1), name='W')\n",
    "                conv2d_1_biases = tf.Variable(tf.constant(1.0, shape=[depth[0]]), name='B')\n",
    "\n",
    "                conv = tf.nn.conv2d(tf_train_dataset, conv2d_1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                hidden = tf.nn.relu(conv + conv2d_1_biases)\n",
    "                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "                tf.summary.histogram(\"weights\", conv2d_1_weights)\n",
    "                tf.summary.histogram(\"biases\", conv2d_1_biases)\n",
    "                tf.summary.histogram(\"activations\", hidden)\n",
    "                tf.summary.histogram(\"pooling\", pool)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        # Reshape convolution layers to process the nodes further with connected layers\n",
    "        with tf.name_scope('reshape'):\n",
    "            shape = pool.get_shape().as_list()\n",
    "            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    \n",
    "\n",
    "        if layer and layer[0]=='f':\n",
    "            layer = layer[1:]        \n",
    "            with tf.name_scope('fc_1'):\n",
    "                shape = output.get_shape().as_list()\n",
    "                fc_1_weights = tf.Variable(tf.truncated_normal([shape[1], num_hidden], stddev=0.1), name='W')\n",
    "                fc_1_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_1_weights) + fc_1_biases)\n",
    "\n",
    "                tf.summary.histogram(\"weights\", fc_1_weights)\n",
    "                tf.summary.histogram(\"biases\", fc_1_biases)\n",
    "                tf.summary.histogram(\"activations\", output)\n",
    "                \n",
    "                \n",
    "\n",
    "        if layer and layer[0]=='f':\n",
    "            layer = layer[1:]        \n",
    "            with tf.name_scope('fc_2'):\n",
    "                shape = output.get_shape().as_list()\n",
    "                fc_2_weights = tf.Variable(tf.truncated_normal([shape[1], num_hidden], stddev=0.1), name='W')\n",
    "                fc_2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_2_weights) + fc_2_biases)\n",
    "\n",
    "                tf.summary.histogram(\"weights\", fc_2_weights)\n",
    "                tf.summary.histogram(\"biases\", fc_2_biases)\n",
    "                tf.summary.histogram(\"activations\", output)\n",
    "                \n",
    "                \n",
    "\n",
    "        if layer and layer[0]=='f':\n",
    "            layer = layer[1:]        \n",
    "            with tf.name_scope('fc_3'):\n",
    "                shape = output.get_shape().as_list()\n",
    "                fc_3_weights = tf.Variable(tf.truncated_normal([shape[1], num_hidden], stddev=0.1), name='W')\n",
    "                fc_3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_3_weights) + fc_3_biases)\n",
    "\n",
    "                tf.summary.histogram(\"weights\", fc_3_weights)\n",
    "                tf.summary.histogram(\"biases\", fc_3_biases)\n",
    "                tf.summary.histogram(\"activations\", output)\n",
    "                \n",
    "                \n",
    "\n",
    "        if layer and layer[0]=='f':\n",
    "            layer = layer[1:]        \n",
    "            with tf.name_scope('fc_4'):\n",
    "                shape = output.get_shape().as_list()\n",
    "                fc_4_weights = tf.Variable(tf.truncated_normal([shape[1], num_hidden], stddev=0.1), name='W')\n",
    "                fc_4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_4_weights) + fc_4_biases)\n",
    "\n",
    "                tf.summary.histogram(\"weights\", fc_4_weights)\n",
    "                tf.summary.histogram(\"biases\", fc_4_biases)\n",
    "                tf.summary.histogram(\"activations\", output)\n",
    "                \n",
    "                \n",
    "\n",
    "       \n",
    "        with tf.name_scope('output'):\n",
    "            shape = output.get_shape().as_list()\n",
    "            output_weights = tf.Variable(tf.truncated_normal([shape[1], num_labels], stddev=0.1), name='W')\n",
    "            output_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "            output = tf.matmul(output, output_weights) + output_biases\n",
    "\n",
    "            tf.summary.histogram(\"weights\", output_weights)\n",
    "            tf.summary.histogram(\"biases\", output_biases)\n",
    "            tf.summary.histogram(\"activations\", output)\n",
    "                \n",
    "    \n",
    "    \n",
    "        # Computing the loss of the model\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "        # Optimizing the model\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "        # Predictions for the training, validation, and test data\n",
    "        with tf.name_scope('prediction'):\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(train_prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('batch_accuracy', accuracy)\n",
    "                                                    \n",
    "                \n",
    "        \n",
    "        # Computing the validation-dataset\n",
    "        with tf.name_scope('validation'):\n",
    "            layer = model_name[:-1]\n",
    "\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, conv2d_1_weights, [1, 1, 1, 1], padding='SAME') + conv2d_1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_2_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_3_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_4_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_5_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_6_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool.get_shape().as_list()\n",
    "            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_1_weights) + fc_1_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_2_weights) + fc_2_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_3_weights) + fc_3_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_4_weights) + fc_4_biases)\n",
    "            valid_prediction = tf.nn.softmax(tf.matmul(output, output_weights) + output_biases)\n",
    "                                \n",
    "            correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(valid_labels, 1))\n",
    "            valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('validation_accuracy', valid_accuracy)                        \n",
    "                                \n",
    "        with tf.name_scope('auc'):\n",
    "            valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "            tf.summary.scalar('validation_auc_0', valid_auc[0])\n",
    "            #tf.summary.scalar('validation_auc_1', valid_auc[1])\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Computing the test-dataset\n",
    "        with tf.name_scope('testing'):\n",
    "            layer = model_name[:-1]\n",
    "\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset_final, conv2d_1_weights, [1, 1, 1, 1], padding='SAME') + conv2d_1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_2_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_3_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_4_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_5_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if layer and layer[0]=='c':\n",
    "                layer = layer[1:]\n",
    "                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, conv2d_6_weights, [1, 1, 1, 1], padding='SAME')  + conv2d_6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            shape = pool.get_shape().as_list()\n",
    "            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_1_weights) + fc_1_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_2_weights) + fc_2_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_3_weights) + fc_3_biases)\n",
    "            if layer and layer[0]=='f':\n",
    "                layer = layer[1:]\n",
    "                output = tf.nn.relu(tf.matmul(output, fc_4_weights) + fc_4_biases)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(output, output_weights) + output_biases)\n",
    "                                \n",
    "            test_correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(test_correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('validation_accuracy', test_accuracy)                        \n",
    "                                \n",
    "        with tf.name_scope('auc'):\n",
    "            test_auc = tf.metrics.auc(labels=tf_test_labels_final, predictions=test_prediction, curve='ROC')\n",
    "            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "            #tf.summary.scalar('validation_auc_1', test_auc[1])\n",
    "            \n",
    "    \n",
    "        # Merge all summaries and create a saver\n",
    "        summ = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Initializing the model-variables and specify the logfiles\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(LOGDIR+'/'+logcount+hparams)\n",
    "        writer.add_graph(sess.graph)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Iterating over num_steps batches and train the model \n",
    "        gen = batchYielder()\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = next(gen)\n",
    "            # Creating a feed_dict to train the model on in this step\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            # Train the model for this step\n",
    "            _, l, predictions = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "            # Updating the output to stay in touch with the training process\n",
    "            # Checking for early-stopping with scikit-learn\n",
    "            if (step % 100 == 0):\n",
    "                s = sess.run(summ, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "                writer.add_summary(s, step)\n",
    "                \n",
    "                # Compute the accuracy and the roc-auc-score with scikit-learn\n",
    "                pred = sess.run(valid_prediction)\n",
    "                pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "                stop_acc = accuracy_score(np.argmax(valid_labels, axis=1), np.argmax(pred, axis=1))\n",
    "                stop_auc = roc_auc_score(valid_labels, pred)\n",
    "                      \n",
    "                  \n",
    "                # Check if early-stopping is necessary\n",
    "                auc_now = stop_auc\n",
    "                if step == 0:\n",
    "                    stopping_auc = 0.0\n",
    "                    sink_count = 0\n",
    "                else:\n",
    "                    if auc_now > stopping_auc:\n",
    "                        stopping_auc = auc_now\n",
    "                        sink_count = 0\n",
    "                        # Check if the model is better than the existing one and has to be saved\n",
    "                        if stopping_auc > best_auc:\n",
    "                            saver.save(sess, os.path.join(folder_path, model_name))\n",
    "                            best_auc = stopping_auc\n",
    "                    else:\n",
    "                        sink_count += 1\n",
    "                        \n",
    "                # Printing a current evaluation of the model\n",
    "                print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, stop_acc*100, step))\n",
    "                if sink_count == 10:\n",
    "                    break   \n",
    "    \n",
    "        \n",
    "        # Compute the final score of the model       \n",
    "        pred = sess.run(test_prediction)\n",
    "        pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "        f_acc = accuracy_score(np.argmax(test_labels, axis=1), np.argmax(pred, axis=1))\n",
    "        f_auc = roc_auc_score(test_labels, pred)\n",
    "        \n",
    "        # Close the session\n",
    "        sess.close()\n",
    "        \n",
    "        print('Final_auc: {}, Final_acc: {}'.format(f_auc, f_acc))\n",
    "        # Save the run to the csv and restart the loop\n",
    "        dauer = time.time() - start\n",
    "        early_stopped = True if step < num_steps-1 else False\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([learning_rate, batch_size, patch_size, *depth, num_hidden, f_acc*100, f_auc, step, early_stopped, dauer, title_name])\n",
    "\n",
    "    except:\n",
    "        sess.close()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
