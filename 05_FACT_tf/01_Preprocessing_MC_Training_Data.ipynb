{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Retrieving all MC-Data and storing it in matrices\n",
    "\n",
    "To reduce computation time the data will be preprocessed to fit into the CNN. First of all the photon arrival times are loaded from the server and are shaped to fit into a flat 2d-matrix.\n",
    "\n",
    "Execute this script only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getMetadata(load_metadata=True):\n",
    "    '''\n",
    "    Loads/gathers the file paths and the number of contained events\n",
    "    '''\n",
    "    \n",
    "    file_path = '01_File_event_count.csv'\n",
    "    \n",
    "    if load_metadata:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "    else:\n",
    "        # Path to the directory containing subdirectories and all datafile\n",
    "        main_path = '/net/big-tank/POOL/projects/fact/simulation/photon_stream/fact_tools/v.0.18.0/'\n",
    "\n",
    "        # Iterate over every file in the subdirs and check if it has the right file extension\n",
    "        file_paths = [os.path.join(dirPath, file) for dirPath, dirName, fileName in os.walk(os.path.expanduser(main_path)) for file in fileName if '.json' in file]\n",
    "        \n",
    "        # Count numbers of files in every subdir\n",
    "        proton_files = []\n",
    "        gustav_files = []\n",
    "        werner_files = []\n",
    "        fehler_files = []\n",
    "\n",
    "        for file in file_paths:\n",
    "            if 'proton' in file:\n",
    "                proton_files.append(file)\n",
    "            elif 'gustav' in file:\n",
    "                gustav_files.append(file)\n",
    "            elif 'werner' in file:\n",
    "                werner_files.append(file)\n",
    "            else: fehler_files.append(file)\n",
    "        \n",
    "        # Count every element in every file\n",
    "        events = []\n",
    "        for subdir in [proton_files, gustav_files, werner_files]:\n",
    "            file_list = []\n",
    "            for file in subdir:\n",
    "                event_count = 0\n",
    "                with gzip.open(file) as event_data:\n",
    "                    for event in event_data:\n",
    "                        event_count += 1\n",
    "                file_list.append([file, event_count])\n",
    "            events.append(file_list)\n",
    "        \n",
    "        data = []\n",
    "        for elem in events:\n",
    "            for i in elem:\n",
    "                data.append(i)\n",
    "                \n",
    "        # Save metadata to a df\n",
    "        df = pd.DataFrame(data, columns=['File_name', 'Event_count'])\n",
    "        df['Particle'] = df['File_name'].apply(lambda x: False if 'proton' in x else True)\n",
    "        df.to_csv(file_path, encoding='utf-8', index=False)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Every gzip-file will be opened and the contained information will be reshaped with the mapping of the hexagonal-position dictionary. Batches of 1000 events will be stored in separated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "df = getMetadata(load_metadata=True)\n",
    "id_position = pickle.load(open(\"01_hexagonal_position_dict.p\", \"rb\"))\n",
    "\n",
    "data = []\n",
    "num = 0\n",
    "for elem in df.values:\n",
    "    with gzip.open(elem[0]) as file:\n",
    "        for line in file:\n",
    "            event_photons = json.loads(line.decode('utf-8'))['PhotonArrivals_500ps']\n",
    "            \n",
    "            input_matrix = np.zeros([46,45])\n",
    "            for i in range(1440):\n",
    "                x, y = id_position[i]\n",
    "                input_matrix[int(x)][int(y)] = len(event_photons[i])\n",
    "            \n",
    "            data.append([input_matrix, elem[2]])\n",
    "            \n",
    "            if len(data)%1000 == 0:\n",
    "                with gzip.open( \"/fhgfs/users/jbehnken/01_Data/99_Temporary/PhotonArrivals_500ps_\"+str(num)+\".p\", \"wb\" ) as data_file:\n",
    "                    pickle.dump(data, data_file)\n",
    "                data = []\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Formating the preprocessed data\n",
    "\n",
    "Every file will be opened, the data will be converted to np.arrays and pictures and labels will be stored together in a dictionary. The resulting files will still contain 1000 events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "path = '/fhgfs/users/jbehnken/01_Data/99_Temporary'\n",
    "path_new = '/fhgfs/users/jbehnken/01_Data/99_Temporary'\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 46, 45, 1)).astype(np.float32)\n",
    "    labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "def rewrite(file):\n",
    "        with gzip.open(path+'/'+file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                pic, lab = zip(*data)\n",
    "                pic, lab = reformat(np.array(pic), np.array(lab))\n",
    "\n",
    "        data_dict={'Image':pic, 'Label':lab}\n",
    "\n",
    "        with gzip.open(path_new+'/'+file, 'wb') as f:\n",
    "                pickle.dump(data_dict, f)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "p = Pool()\n",
    "data = p.map(rewrite, os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Randomizing and standardizing the formated data\n",
    "\n",
    "All files will be loaded into memory, then they will be shuffled, standardized and stored again into the same data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24904 2.36506\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Path to preprocessed data\n",
    "path = '/fhgfs/users/jbehnken/01_Data/99_Temporary'\n",
    "\n",
    "\n",
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, os.listdir(path))\n",
    "pics, labs = zip(*data)\n",
    "del data, p\n",
    "\n",
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "\n",
    "# Values to standardize the data\n",
    "mean = np.mean(pic)\n",
    "std = np.std(pic)\n",
    "print(mean, std)\n",
    "\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "all_pics = pic[p]\n",
    "all_labels = lab[p]\n",
    "del p, pic, lab\n",
    "\n",
    "def save_data(i):\n",
    "    pics_batch = all_pics[(i-1)*1000:i*1000]\n",
    "    labels_batch = all_labels[(i-1)*1000:i*1000]\n",
    "    \n",
    "    data_dict={'Image':(pics_batch-mean)/std, 'Label':labels_batch}\n",
    "    with gzip.open('/fhgfs/users/jbehnken/01_Data/01_MC_Data/PhotonArrivals_500ps_{}.p'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "        \n",
    "num_files = len(os.listdir(path))\n",
    "p = Pool()\n",
    "data = p.map(save_data, range(1,num_files+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "mean: 1.24904\n",
    "\n",
    "std: 2.36506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Result\n",
    "\n",
    "The 2.422.000 events are standardized, randomized and saved to the disc in files containing 1000 single events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = '01_File_event_count.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "main_path = '/net/big-tank/POOL/projects/fact/simulation/photon_stream/fact_tools/v.0.18.0/'\n",
    "# Iterate over every file in the subdirs and check if it has the right file extension\n",
    "file_paths = [os.path.join(dirPath, file) for dirPath, dirName, fileName in os.walk(os.path.expanduser(main_path)) for file in fileName if '.json' in file]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>Event_count</th>\n",
       "      <th>Particle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/net/big-tank/POOL/projects/fact/simulation/ph...</td>\n",
       "      <td>1322</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_name  Event_count Particle\n",
       "0  /net/big-tank/POOL/projects/fact/simulation/ph...         1322    False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/big-tank/POOL/projects/fact/simulation/photon_stream/fact_tools/v.0.18.0/gamma_gustav_12/output_spe_extractor_mc_39.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
