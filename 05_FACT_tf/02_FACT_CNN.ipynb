{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACT CNN\n",
    "\n",
    "The randomized data will be loaded into memory, split into training-, validation- and testing-set. Afterwards a CNN with two convolution layers and two fully connected layers will be trained. The training process will be documented with accuracy- and auc-values in Tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading randomized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/fhgfs/users/jbehnken/01_Data/_01_MC_Data' # Path to preprocessed data\n",
    "num_files = 100\n",
    "\n",
    "# Load pickled data and split it into pictures and labels\n",
    "def load_data(file):\n",
    "    with gzip.open(path+'/'+file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    pic = data_dict['Image']\n",
    "    lab = data_dict['Label']\n",
    "    return (pic, lab)\n",
    "\n",
    "# Randomizing the files to load\n",
    "loading_files = os.listdir(path)\n",
    "np.random.shuffle(loading_files)\n",
    "\n",
    "# Pool-load pickled data and split it into pictures and labels (list)\n",
    "p = Pool()\n",
    "data = p.map(load_data, loading_files[:num_files])\n",
    "pics, labs = zip(*data)\n",
    "del data, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data to a single np.array\n",
    "pic = np.concatenate(pics)\n",
    "lab = np.concatenate(labs)\n",
    "del pics, labs\n",
    "\n",
    "# Randomize and split the data into train/validation/test dataset\n",
    "p = np.random.permutation(len(pic))\n",
    "\n",
    "valid_dataset = pic[p][:10000]\n",
    "valid_labels = lab[p][:10000]\n",
    "test_dataset = pic[p][10000:50000]\n",
    "test_labels = lab[p][10000:50000]\n",
    "train_dataset = pic[p][50000:]\n",
    "train_labels = lab[p][50000:]\n",
    "del p, pic, lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St_auc: 0.0, sc: 0,val: 78.27000021934509, Step: 0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter for the model (fit manually)\n",
    "num_labels = 2 # gamma or proton\n",
    "num_channels = 1 # it is a greyscale image\n",
    "\n",
    "# Quantity of batches to train with\n",
    "for num_steps in [20001]:\n",
    "    \n",
    "    # Different learning rates for the optimizer\n",
    "    for learning_rate in [ 0.001]:\n",
    "        \n",
    "        # Quantity of the events in one batch\n",
    "        for batch_size in [32, 64, 128, 256]:\n",
    "            \n",
    "            # Size of the conv 2d filter [ixi]\n",
    "            for patch_size in [3, 5]:\n",
    "                \n",
    "                # Quantity of the filters of the first conv2d-layer\n",
    "                for depth in  [8, 16, 32]:\n",
    "                    \n",
    "                    # Quantity of the nodes in the first hidden layer\n",
    "                    for num_hidden in  [16, 32, 64, 128, 256]:\n",
    "\n",
    "                        # Path to logfiles and correct file name\n",
    "                        LOGDIR = '/fhgfs/users/jbehnken/tf_logs/small_logs'\n",
    "                        logcount = str(len(os.listdir(LOGDIR)))\n",
    "                        hparams = 'Adam_lr={}_bs={}_ps={}_d={}_nh={}_ns={}'.format(learning_rate, batch_size, patch_size, depth, num_hidden, num_steps)\n",
    "\n",
    "                        \n",
    "                        # Build the graph\n",
    "                        tf.reset_default_graph()\n",
    "                        sess = tf.Session()\n",
    "\n",
    "                        \n",
    "                        # Create tf.variables for the three different datasets\n",
    "                        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='training_data')\n",
    "                        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='training_labels')\n",
    "                        \n",
    "                        #tf.summary.image('input', tf_train_dataset, 6)\n",
    "                        \n",
    "                        tf_valid_dataset = tf.constant(valid_dataset, name='validation_data')\n",
    "                        tf_valid_labels = tf.constant(valid_labels, name='validation_labels')\n",
    "                        tf_test_dataset = tf.constant(test_dataset, name='testing_data')\n",
    "                        tf_test_labels = tf.constant(test_labels, name='testing_labels')\n",
    "\n",
    "\n",
    "                        # First layer is a convolution layer\n",
    "                        with tf.name_scope('conv2d_1'):\n",
    "                            layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1), name='W')\n",
    "                            layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth]), name='B')\n",
    "\n",
    "                            conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "                            #tf.summary.histogram(\"weights\", layer1_weights)\n",
    "                            #tf.summary.histogram(\"biases\", layer1_biases)\n",
    "                            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "\n",
    "                        # Second layer is a convolution layer\n",
    "                        with tf.name_scope('conv2d_2'):\n",
    "                            layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, 2*depth], stddev=0.1), name='W')\n",
    "                            layer2_biases = tf.Variable(tf.constant(1.0, shape=[2*depth]), name='B')\n",
    "\n",
    "                            conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME') \n",
    "                            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                            pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "                            #tf.summary.histogram(\"weights\", layer2_weights)\n",
    "                            #tf.summary.histogram(\"biases\", layer2_biases)\n",
    "                            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "\n",
    "                        # The reshape produces an input vector for the dense layer\n",
    "                        with tf.name_scope('reshape'):\n",
    "                            shape = pool.get_shape().as_list()\n",
    "                            reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "\n",
    "                        # Third layer is a dense layer\n",
    "                        with tf.name_scope('fc_1'):\n",
    "                            layer3_weights = tf.Variable(tf.truncated_normal([12*12*2*depth, num_hidden], stddev=0.1), name='W')\n",
    "                            layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "\n",
    "                            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "                            #tf.summary.histogram(\"weights\", layer3_weights)\n",
    "                            #tf.summary.histogram(\"biases\", layer3_biases)\n",
    "                            #tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "\n",
    "                        # Fourth layer is a dense output layer\n",
    "                        with tf.name_scope('fc_2'):\n",
    "                            layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name='W')\n",
    "                            layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "\n",
    "                            output = tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "                            #tf.summary.histogram(\"weights\", layer4_weights)\n",
    "                            #tf.summary.histogram(\"biases\", layer4_biases)\n",
    "                            #tf.summary.histogram(\"activations\", output)\n",
    "\n",
    "\n",
    "                        # Computing the loss of the model\n",
    "                        with tf.name_scope('loss'):\n",
    "                            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "                            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "\n",
    "                        # Optimizing the model\n",
    "                        with tf.name_scope('optimizer'):\n",
    "                            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "                        # Predictions for the training, validation, and test data\n",
    "                        with tf.name_scope('prediction'):\n",
    "                            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "\n",
    "                        with tf.name_scope('accuracy'):\n",
    "                            correct_prediction = tf.equal(tf.argmax(train_prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "                            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                            tf.summary.scalar('batch_accuracy', accuracy)\n",
    "                            \n",
    "                            \n",
    "                        with tf.name_scope('validation'):\n",
    "                            pool_1 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            pool_2 = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME')  + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            shape = pool_2.get_shape().as_list()\n",
    "                            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "                            valid_prediction = tf.nn.softmax(tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "                            \n",
    "                            correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(valid_labels, 1))\n",
    "                            valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                            tf.summary.scalar('validation_accuracy', valid_accuracy)\n",
    "                            \n",
    "                            \n",
    "                        with tf.name_scope('auc'):\n",
    "                            auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                            tf.summary.scalar('validation_auc_0', auc[0])\n",
    "                            #tf.summary.scalar('validation_auc_1', auc[1])\n",
    "\n",
    "\n",
    "                        summ = tf.summary.merge_all()\n",
    "                        saver = tf.train.Saver()\n",
    "\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        writer = tf.summary.FileWriter(LOGDIR+'/'+logcount+hparams)\n",
    "                        writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "                        # Iterating over num_setps \n",
    "                        for step in range(num_steps):\n",
    "                            # Computing the offset to move over the training dataset\n",
    "                            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                            # Getting the batchdata\n",
    "                            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "                            # Getting the batchlabels\n",
    "                            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                            # Creating a feed_dict to train the model on in this step\n",
    "                            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                            # Train the model for this step\n",
    "                            _, l, predictions = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "                            # Updating the output to stay in touch with the training process\n",
    "                            if (step % 50 == 0):\n",
    "                                [acc, val, auc_val, s] = sess.run([accuracy, valid_accuracy, auc, summ], feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "                                #print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                                #print('Minibatch accuracy: %.1f%%' % (acc*100))\n",
    "                                #print('Validation accuracy: %.1f%%' % (val*100))\n",
    "                                #print('Auc: %.2f, %.2f' % (auc_val[0], auc_val[1]))\n",
    "                                #writer.add_summary(s, step)\n",
    "                                \n",
    "                                if step == 0:\n",
    "                                    stopping_auc = 0.0\n",
    "                                    sink_count = 0\n",
    "                                else:\n",
    "                                    if auc_val[0] > stopping_auc:\n",
    "                                        stopping_auc = auc_val[0]\n",
    "                                        sink_count = 0\n",
    "                                    else:\n",
    "                                        sink_count += 1\n",
    "                                print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, val*100, step))\n",
    "                                if sink_count == 5:\n",
    "                                    with open('/home/jbehnken/05_FACT_tf/Hyperparameter.csv', 'a') as f:\n",
    "                                        writer = csv.writer(f)\n",
    "                                        writer.writerow([learning_rate, batch_size, patch_size, depth, num_hidden, val*100, stopping_auc, step])\n",
    "                                    break                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
